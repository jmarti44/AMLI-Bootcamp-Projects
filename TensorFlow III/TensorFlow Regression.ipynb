{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/content/03_regression/08_regression_with_tensorflow/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Copyright 2020 Google LLC."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "copyright"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ],
   "outputs": [],
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "hMqWDc_m6rUC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Regression with TensorFlow"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "c2hPzRb6j_CA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have trained a linear regression model in TensorFlow and used it to predict housing prices. However, the model didn't perform as well as we would have liked it to. In this lab, we will build a neural network to try to tackle the same regression problem and see if we can get better results."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "9x88D_U-4oTH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading and Preparing the Data"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "6AaBjI9hmqnT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset we'll use for this Colab contains California housing information taken from the 1990 census data. We explored this data in a previous lab, so we won't do an analysis here. As a reminder, the documentation for the dataset can be found [on Kaggle](https://www.kaggle.com/camnugent/california-housing-prices)."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "4f3CKqFUqL2-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Upload your `kaggle.json` file and run the code block below."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "MxiIKhP4E2Zr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "source": [
    "! chmod 600 kaggle.json && (ls ~/.kaggle 2>/dev/null || mkdir ~/.kaggle) && mv kaggle.json ~/.kaggle/ && echo 'Done'"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "chmod: kaggle.json: No such file or directory\n"
     ]
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tkRBt4c813MP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once you are done, use the `kaggle` command to download the file into the lab."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "WdltN0vY18H_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "source": [
    "!kaggle datasets download camnugent/california-housing-prices\n",
    "!ls"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "california-housing-prices.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "california-housing-prices.zip slides.md\n",
      "colab-key.zip                 slides.pptx\n",
      "colab.ipynb\n"
     ]
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wt4pCnny2Am2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now have a file called `california-housing-prices.zip` that we can load into a `DataFrame`."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "5aJqOuak2FCs"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "housing_df = pd.read_csv('california-housing-prices.zip')\n",
    "\n",
    "housing_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>-121.09</td>\n",
       "      <td>39.48</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1665.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>845.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>1.5603</td>\n",
       "      <td>78100.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>-121.21</td>\n",
       "      <td>39.49</td>\n",
       "      <td>18.0</td>\n",
       "      <td>697.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>2.5568</td>\n",
       "      <td>77100.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>-121.22</td>\n",
       "      <td>39.43</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>1.7000</td>\n",
       "      <td>92300.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>-121.32</td>\n",
       "      <td>39.43</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1860.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>741.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>1.8672</td>\n",
       "      <td>84700.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>-121.24</td>\n",
       "      <td>39.37</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2785.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>530.0</td>\n",
       "      <td>2.3886</td>\n",
       "      <td>89400.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0        -122.23     37.88                41.0        880.0           129.0   \n",
       "1        -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2        -122.24     37.85                52.0       1467.0           190.0   \n",
       "3        -122.25     37.85                52.0       1274.0           235.0   \n",
       "4        -122.25     37.85                52.0       1627.0           280.0   \n",
       "...          ...       ...                 ...          ...             ...   \n",
       "20635    -121.09     39.48                25.0       1665.0           374.0   \n",
       "20636    -121.21     39.49                18.0        697.0           150.0   \n",
       "20637    -121.22     39.43                17.0       2254.0           485.0   \n",
       "20638    -121.32     39.43                18.0       1860.0           409.0   \n",
       "20639    -121.24     39.37                16.0       2785.0           616.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \\\n",
       "0           322.0       126.0         8.3252            452600.0   \n",
       "1          2401.0      1138.0         8.3014            358500.0   \n",
       "2           496.0       177.0         7.2574            352100.0   \n",
       "3           558.0       219.0         5.6431            341300.0   \n",
       "4           565.0       259.0         3.8462            342200.0   \n",
       "...           ...         ...            ...                 ...   \n",
       "20635       845.0       330.0         1.5603             78100.0   \n",
       "20636       356.0       114.0         2.5568             77100.0   \n",
       "20637      1007.0       433.0         1.7000             92300.0   \n",
       "20638       741.0       349.0         1.8672             84700.0   \n",
       "20639      1387.0       530.0         2.3886             89400.0   \n",
       "\n",
       "      ocean_proximity  \n",
       "0            NEAR BAY  \n",
       "1            NEAR BAY  \n",
       "2            NEAR BAY  \n",
       "3            NEAR BAY  \n",
       "4            NEAR BAY  \n",
       "...               ...  \n",
       "20635          INLAND  \n",
       "20636          INLAND  \n",
       "20637          INLAND  \n",
       "20638          INLAND  \n",
       "20639          INLAND  \n",
       "\n",
       "[20640 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ivCDWnwE2Zx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we can define which columns are features and which is the target.\n",
    "\n",
    "We'll also make a separate list of our numeric columns."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "KI9ElSbPje6j"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "target_column = 'median_house_value'\n",
    "feature_columns = [c for c in housing_df.columns if c != target_column]\n",
    "numeric_feature_columns = [c for c in feature_columns if c != 'ocean_proximity']\n",
    "\n",
    "target_column, feature_columns, numeric_feature_columns"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('median_house_value',\n",
       " ['longitude',\n",
       "  'latitude',\n",
       "  'housing_median_age',\n",
       "  'total_rooms',\n",
       "  'total_bedrooms',\n",
       "  'population',\n",
       "  'households',\n",
       "  'median_income',\n",
       "  'ocean_proximity'],\n",
       " ['longitude',\n",
       "  'latitude',\n",
       "  'housing_median_age',\n",
       "  'total_rooms',\n",
       "  'total_bedrooms',\n",
       "  'population',\n",
       "  'households',\n",
       "  'median_income'])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTjb8Gn-92v2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also reduced the value of our targets by a factor in the previous lab. This reduction in magnitude was done to help the model train faster. Let's do that again."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "wrOyGuN_k74i"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "TARGET_FACTOR = 100000\n",
    "\n",
    "housing_df[target_column] /= TARGET_FACTOR\n",
    "\n",
    "housing_df[target_column].describe()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "count    20640.000000\n",
       "mean         2.068558\n",
       "std          1.153956\n",
       "min          0.149990\n",
       "25%          1.196000\n",
       "50%          1.797000\n",
       "75%          2.647250\n",
       "max          5.000010\n",
       "Name: median_house_value, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1RkwE7QGlAjG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we filled in some missing `total_bedrooms` values."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "EzTdkcVS2ZCk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "has_all_data = housing_df[~housing_df['total_bedrooms'].isna()]\n",
    "\n",
    "sums = has_all_data[['total_bedrooms', 'total_rooms']].sum().tolist()\n",
    "\n",
    "bedrooms_to_total_rooms_ratio = sums[0] / sums[1]\n",
    "\n",
    "missing_total_bedrooms_idx = housing_df['total_bedrooms'].isna()\n",
    "\n",
    "housing_df.loc[missing_total_bedrooms_idx, 'total_bedrooms'] = housing_df[\n",
    "    missing_total_bedrooms_idx]['total_rooms'] * bedrooms_to_total_rooms_ratio\n",
    "\n",
    "housing_df.describe()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-119.569704</td>\n",
       "      <td>35.631861</td>\n",
       "      <td>28.639486</td>\n",
       "      <td>2635.763081</td>\n",
       "      <td>537.719351</td>\n",
       "      <td>1425.476744</td>\n",
       "      <td>499.539680</td>\n",
       "      <td>3.870671</td>\n",
       "      <td>2.068558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.003532</td>\n",
       "      <td>2.135952</td>\n",
       "      <td>12.585558</td>\n",
       "      <td>2181.615252</td>\n",
       "      <td>420.848774</td>\n",
       "      <td>1132.462122</td>\n",
       "      <td>382.329753</td>\n",
       "      <td>1.899822</td>\n",
       "      <td>1.153956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-124.350000</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.499900</td>\n",
       "      <td>0.149990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-121.800000</td>\n",
       "      <td>33.930000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1447.750000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>787.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>2.563400</td>\n",
       "      <td>1.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-118.490000</td>\n",
       "      <td>34.260000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2127.000000</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>1166.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>3.534800</td>\n",
       "      <td>1.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-118.010000</td>\n",
       "      <td>37.710000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>3148.000000</td>\n",
       "      <td>647.000000</td>\n",
       "      <td>1725.000000</td>\n",
       "      <td>605.000000</td>\n",
       "      <td>4.743250</td>\n",
       "      <td>2.647250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-114.310000</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>39320.000000</td>\n",
       "      <td>6445.000000</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>6082.000000</td>\n",
       "      <td>15.000100</td>\n",
       "      <td>5.000010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          longitude      latitude  housing_median_age   total_rooms  \\\n",
       "count  20640.000000  20640.000000        20640.000000  20640.000000   \n",
       "mean    -119.569704     35.631861           28.639486   2635.763081   \n",
       "std        2.003532      2.135952           12.585558   2181.615252   \n",
       "min     -124.350000     32.540000            1.000000      2.000000   \n",
       "25%     -121.800000     33.930000           18.000000   1447.750000   \n",
       "50%     -118.490000     34.260000           29.000000   2127.000000   \n",
       "75%     -118.010000     37.710000           37.000000   3148.000000   \n",
       "max     -114.310000     41.950000           52.000000  39320.000000   \n",
       "\n",
       "       total_bedrooms    population    households  median_income  \\\n",
       "count    20640.000000  20640.000000  20640.000000   20640.000000   \n",
       "mean       537.719351   1425.476744    499.539680       3.870671   \n",
       "std        420.848774   1132.462122    382.329753       1.899822   \n",
       "min          1.000000      3.000000      1.000000       0.499900   \n",
       "25%        295.000000    787.000000    280.000000       2.563400   \n",
       "50%        435.000000   1166.000000    409.000000       3.534800   \n",
       "75%        647.000000   1725.000000    605.000000       4.743250   \n",
       "max       6445.000000  35682.000000   6082.000000      15.000100   \n",
       "\n",
       "       median_house_value  \n",
       "count        20640.000000  \n",
       "mean             2.068558  \n",
       "std              1.153956  \n",
       "min              0.149990  \n",
       "25%              1.196000  \n",
       "50%              1.797000  \n",
       "75%              2.647250  \n",
       "max              5.000010  "
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LByQBpsy2b--"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 1: Standardization\n",
    "\n",
    "Previously when we worked with this dataset, we normalized the feature data in order to get it ready for the model. Normalization was the process of making all of the data fit between 0.0 and 1.0 by subtracting the minimum of each column from each data point in that column and then dividing by the delta between the maximum and minimum values.\n",
    "\n",
    "In this exercise you will need to standardize all of the feature columns. Standardization is performed by subtracting the mean value of each column from each data point in that column and then dividing by the standard deviation.\n",
    "\n",
    "> *Hint: When you are done call `describe()` and ensure that the standard deviation for every feature column is 1.0`*"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "YYI2A_97jlD-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "housing_df.loc[:, numeric_feature_columns] = (\n",
    "    housing_df[numeric_feature_columns] - \n",
    "      housing_df[numeric_feature_columns].mean()) / (\n",
    "          housing_df[numeric_feature_columns].std())\n",
    "housing_df[numeric_feature_columns].describe()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.064000e+04</td>\n",
       "      <td>2.064000e+04</td>\n",
       "      <td>2.064000e+04</td>\n",
       "      <td>2.064000e+04</td>\n",
       "      <td>2.064000e+04</td>\n",
       "      <td>2.064000e+04</td>\n",
       "      <td>2.064000e+04</td>\n",
       "      <td>2.064000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-1.429215e-12</td>\n",
       "      <td>-7.636681e-14</td>\n",
       "      <td>1.817399e-15</td>\n",
       "      <td>-9.590802e-17</td>\n",
       "      <td>3.758299e-17</td>\n",
       "      <td>-2.836528e-16</td>\n",
       "      <td>7.563865e-17</td>\n",
       "      <td>-2.526564e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.385935e+00</td>\n",
       "      <td>-1.447533e+00</td>\n",
       "      <td>-2.196127e+00</td>\n",
       "      <td>-1.207254e+00</td>\n",
       "      <td>-1.275326e+00</td>\n",
       "      <td>-1.256092e+00</td>\n",
       "      <td>-1.303952e+00</td>\n",
       "      <td>-1.774256e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.113182e+00</td>\n",
       "      <td>-7.967694e-01</td>\n",
       "      <td>-8.453727e-01</td>\n",
       "      <td>-5.445566e-01</td>\n",
       "      <td>-5.767377e-01</td>\n",
       "      <td>-5.637952e-01</td>\n",
       "      <td>-5.742155e-01</td>\n",
       "      <td>-6.881019e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.389006e-01</td>\n",
       "      <td>-6.422715e-01</td>\n",
       "      <td>2.864502e-02</td>\n",
       "      <td>-2.332048e-01</td>\n",
       "      <td>-2.440766e-01</td>\n",
       "      <td>-2.291262e-01</td>\n",
       "      <td>-2.368104e-01</td>\n",
       "      <td>-1.767908e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.784775e-01</td>\n",
       "      <td>9.729330e-01</td>\n",
       "      <td>6.642943e-01</td>\n",
       "      <td>2.347971e-01</td>\n",
       "      <td>2.596673e-01</td>\n",
       "      <td>2.644885e-01</td>\n",
       "      <td>2.758360e-01</td>\n",
       "      <td>4.592952e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.625216e+00</td>\n",
       "      <td>2.957996e+00</td>\n",
       "      <td>1.856137e+00</td>\n",
       "      <td>1.681517e+01</td>\n",
       "      <td>1.403659e+01</td>\n",
       "      <td>3.024960e+01</td>\n",
       "      <td>1.460117e+01</td>\n",
       "      <td>5.858144e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          longitude      latitude  housing_median_age   total_rooms  \\\n",
       "count  2.064000e+04  2.064000e+04        2.064000e+04  2.064000e+04   \n",
       "mean  -1.429215e-12 -7.636681e-14        1.817399e-15 -9.590802e-17   \n",
       "std    1.000000e+00  1.000000e+00        1.000000e+00  1.000000e+00   \n",
       "min   -2.385935e+00 -1.447533e+00       -2.196127e+00 -1.207254e+00   \n",
       "25%   -1.113182e+00 -7.967694e-01       -8.453727e-01 -5.445566e-01   \n",
       "50%    5.389006e-01 -6.422715e-01        2.864502e-02 -2.332048e-01   \n",
       "75%    7.784775e-01  9.729330e-01        6.642943e-01  2.347971e-01   \n",
       "max    2.625216e+00  2.957996e+00        1.856137e+00  1.681517e+01   \n",
       "\n",
       "       total_bedrooms    population    households  median_income  \n",
       "count    2.064000e+04  2.064000e+04  2.064000e+04   2.064000e+04  \n",
       "mean     3.758299e-17 -2.836528e-16  7.563865e-17  -2.526564e-14  \n",
       "std      1.000000e+00  1.000000e+00  1.000000e+00   1.000000e+00  \n",
       "min     -1.275326e+00 -1.256092e+00 -1.303952e+00  -1.774256e+00  \n",
       "25%     -5.767377e-01 -5.637952e-01 -5.742155e-01  -6.881019e-01  \n",
       "50%     -2.440766e-01 -2.291262e-01 -2.368104e-01  -1.767908e-01  \n",
       "75%      2.596673e-01  2.644885e-01  2.758360e-01   4.592952e-01  \n",
       "max      1.403659e+01  3.024960e+01  1.460117e+01   5.858144e+00  "
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8PMD-9e8kVDK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "1tsH6xLZkXFL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### One-Hot Encoding"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "Jt-DiypL6HZ9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `ocean_proximity` column will not work with the neural network model that we are planning to build. Neural networks expect numeric values, but `ocean_proximity` contains string values.\n",
    "\n",
    "Let's remind ourselves which values it contains:"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "qPpALMfd6KbK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "sorted(housing_df['ocean_proximity'].unique())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uYL4unBF6NeC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are five string values. In our linear regression Colab we told TensorFlow to treat these values as a categorical column. Each string was converted to a whole number that represented their position in a vocabulary list: `0`, `1`, `2`, `3`, or `4`.\n",
    "\n",
    "For neural networks it is common to see another strategy called one-hot encoding. One-hot encoding is the process of taking a column with a fixed list of string values and turning it into multiple columns containing only zeros and ones.\n",
    "\n",
    "For instance the column `ocean_proximity` containing five strings would be converted to five columns containing ones and zeros:\n",
    "\n",
    "op_sub_hr | op_inland | op_island | op_near_bay | op_near_ocean\n",
    "----------|-----------|-----------|-------------|--------------\n",
    " 0        | 0         | 0         | 1           | 0\n",
    " 0        | 1         | 0         | 0           | 0\n",
    " 0        | 1         | 0         | 0           | 0\n",
    " 1        | 0         | 0         | 0           | 0\n",
    " 0        | 0         | 1         | 0           | 0\n",
    " 0        | 0         | 0         | 0           | 1\n",
    " 0        | 0         | 1         | 0           | 0\n",
    "\n",
    " Notice that in each row, only one column has a value of `1`. The rest are all `0`. This is the \"one-hot\" in one-hot encoding.\n",
    "\n",
    "As you can imagine, it doesn't scale well for columns with many distinct values. In our case, `5` is perfectly reasonable.\n",
    "\n",
    "Let's manually one-hot encode our data."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "moQf0ZjC6Rso"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "for op in sorted(housing_df['ocean_proximity'].unique()):\n",
    "  op_col = op.lower().replace(' ', '_').replace('<', '')\n",
    "  housing_df[op_col] = (housing_df['ocean_proximity'] == op).astype(int)\n",
    "  feature_columns.append(op_col)\n",
    "\n",
    "feature_columns.remove('ocean_proximity')\n",
    "\n",
    "housing_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>1h_ocean</th>\n",
       "      <th>inland</th>\n",
       "      <th>island</th>\n",
       "      <th>near_bay</th>\n",
       "      <th>near_ocean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.327803</td>\n",
       "      <td>1.052523</td>\n",
       "      <td>0.982119</td>\n",
       "      <td>-0.804800</td>\n",
       "      <td>-0.971179</td>\n",
       "      <td>-0.974405</td>\n",
       "      <td>-0.977009</td>\n",
       "      <td>2.344709</td>\n",
       "      <td>4.526</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.322812</td>\n",
       "      <td>1.043159</td>\n",
       "      <td>-0.607004</td>\n",
       "      <td>2.045841</td>\n",
       "      <td>1.350320</td>\n",
       "      <td>0.861418</td>\n",
       "      <td>1.669921</td>\n",
       "      <td>2.332181</td>\n",
       "      <td>3.585</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.332794</td>\n",
       "      <td>1.038478</td>\n",
       "      <td>1.856137</td>\n",
       "      <td>-0.535733</td>\n",
       "      <td>-0.826233</td>\n",
       "      <td>-0.820757</td>\n",
       "      <td>-0.843616</td>\n",
       "      <td>1.782656</td>\n",
       "      <td>3.521</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.337785</td>\n",
       "      <td>1.038478</td>\n",
       "      <td>1.856137</td>\n",
       "      <td>-0.624199</td>\n",
       "      <td>-0.719307</td>\n",
       "      <td>-0.766010</td>\n",
       "      <td>-0.733764</td>\n",
       "      <td>0.932945</td>\n",
       "      <td>3.413</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.337785</td>\n",
       "      <td>1.038478</td>\n",
       "      <td>1.856137</td>\n",
       "      <td>-0.462393</td>\n",
       "      <td>-0.612380</td>\n",
       "      <td>-0.759828</td>\n",
       "      <td>-0.629142</td>\n",
       "      <td>-0.012881</td>\n",
       "      <td>3.422</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>-0.758808</td>\n",
       "      <td>1.801603</td>\n",
       "      <td>-0.289180</td>\n",
       "      <td>-0.444974</td>\n",
       "      <td>-0.389022</td>\n",
       "      <td>-0.512579</td>\n",
       "      <td>-0.443438</td>\n",
       "      <td>-1.216099</td>\n",
       "      <td>0.781</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>-0.818702</td>\n",
       "      <td>1.806285</td>\n",
       "      <td>-0.845373</td>\n",
       "      <td>-0.888682</td>\n",
       "      <td>-0.921280</td>\n",
       "      <td>-0.944382</td>\n",
       "      <td>-1.008396</td>\n",
       "      <td>-0.691576</td>\n",
       "      <td>0.771</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>-0.823693</td>\n",
       "      <td>1.778194</td>\n",
       "      <td>-0.924829</td>\n",
       "      <td>-0.174991</td>\n",
       "      <td>-0.125269</td>\n",
       "      <td>-0.369528</td>\n",
       "      <td>-0.174037</td>\n",
       "      <td>-1.142566</td>\n",
       "      <td>0.923</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>-0.873605</td>\n",
       "      <td>1.778194</td>\n",
       "      <td>-0.845373</td>\n",
       "      <td>-0.355591</td>\n",
       "      <td>-0.305857</td>\n",
       "      <td>-0.604415</td>\n",
       "      <td>-0.393743</td>\n",
       "      <td>-1.054557</td>\n",
       "      <td>0.847</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>-0.833676</td>\n",
       "      <td>1.750104</td>\n",
       "      <td>-1.004285</td>\n",
       "      <td>0.068407</td>\n",
       "      <td>0.186007</td>\n",
       "      <td>-0.033976</td>\n",
       "      <td>0.079670</td>\n",
       "      <td>-0.780111</td>\n",
       "      <td>0.894</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0      -1.327803  1.052523            0.982119    -0.804800       -0.971179   \n",
       "1      -1.322812  1.043159           -0.607004     2.045841        1.350320   \n",
       "2      -1.332794  1.038478            1.856137    -0.535733       -0.826233   \n",
       "3      -1.337785  1.038478            1.856137    -0.624199       -0.719307   \n",
       "4      -1.337785  1.038478            1.856137    -0.462393       -0.612380   \n",
       "...          ...       ...                 ...          ...             ...   \n",
       "20635  -0.758808  1.801603           -0.289180    -0.444974       -0.389022   \n",
       "20636  -0.818702  1.806285           -0.845373    -0.888682       -0.921280   \n",
       "20637  -0.823693  1.778194           -0.924829    -0.174991       -0.125269   \n",
       "20638  -0.873605  1.778194           -0.845373    -0.355591       -0.305857   \n",
       "20639  -0.833676  1.750104           -1.004285     0.068407        0.186007   \n",
       "\n",
       "       population  households  median_income  median_house_value  \\\n",
       "0       -0.974405   -0.977009       2.344709               4.526   \n",
       "1        0.861418    1.669921       2.332181               3.585   \n",
       "2       -0.820757   -0.843616       1.782656               3.521   \n",
       "3       -0.766010   -0.733764       0.932945               3.413   \n",
       "4       -0.759828   -0.629142      -0.012881               3.422   \n",
       "...           ...         ...            ...                 ...   \n",
       "20635   -0.512579   -0.443438      -1.216099               0.781   \n",
       "20636   -0.944382   -1.008396      -0.691576               0.771   \n",
       "20637   -0.369528   -0.174037      -1.142566               0.923   \n",
       "20638   -0.604415   -0.393743      -1.054557               0.847   \n",
       "20639   -0.033976    0.079670      -0.780111               0.894   \n",
       "\n",
       "      ocean_proximity  1h_ocean  inland  island  near_bay  near_ocean  \n",
       "0            NEAR BAY         0       0       0         1           0  \n",
       "1            NEAR BAY         0       0       0         1           0  \n",
       "2            NEAR BAY         0       0       0         1           0  \n",
       "3            NEAR BAY         0       0       0         1           0  \n",
       "4            NEAR BAY         0       0       0         1           0  \n",
       "...               ...       ...     ...     ...       ...         ...  \n",
       "20635          INLAND         0       1       0         0           0  \n",
       "20636          INLAND         0       1       0         0           0  \n",
       "20637          INLAND         0       1       0         0           0  \n",
       "20638          INLAND         0       1       0         0           0  \n",
       "20639          INLAND         0       1       0         0           0  \n",
       "\n",
       "[20640 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DC2E1ItX6Q5N"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 2: Split the Data\n",
    "\n",
    "We want to hold out some of the data for validation. Using standard Python or a library, split the data. Put 20% of the data in a `DataFrame` called `testing_df` and the other 80% in a `DataFrame` called `training_df`. Be sure to shuffle the data before splitting. Print the number of records in `testing_df` and `training_df` in order to check your work."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "b6RtwkjglSwN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Student Solution**"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "30T4hQ4bmCM-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Shuffle\n",
    "housing_df = housing_df.sample(frac=1)\n",
    "\n",
    "# Calculate test set size\n",
    "test_set_size = int(len(housing_df) * 0.2)\n",
    "\n",
    "# Split the data\n",
    "testing_df = housing_df[:test_set_size]\n",
    "training_df = housing_df[test_set_size:]\n",
    "\n",
    "print(f'Holding out {len(testing_df)} records for testing. ')\n",
    "print(f'Using {len(training_df)} records for training.')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Holding out 4128 records for testing. \n",
      "Using 16512 records for training.\n"
     ]
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-GqS7LRJmFsf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "MIOTjR4mmL_X"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the Model"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "hyCUUMkpn0Wi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will build the model using TensorFlow 2. Let's enable it and go ahead and load up TensorFlow."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "qv4GnnTyohQe"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6u6detBun2pN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we built a TensorFlow `LinearRegressor` in a previous lab, we were using a pre-configured model. For our neural network regressor, we will build the model ourselves using the [Keras API of TensorFlow](https://www.tensorflow.org/guide/keras).\n",
    "\n",
    "We'll build a **sequential** model where one layer feeds into the next. Each layer will be **densely connected**, which means every node in one layer connects to every node in the next layer.\n",
    "\n",
    "A few things are required for our network. We need to have 13 input nodes since that is the number of features that we have (8 original numerical columns, plus the 5 one-hot encoded ocean proximity columns that we added). We also need to have one output node since we are trying to predict a single price value.\n",
    "\n",
    "Let's see what that would look like:\n"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "e4Pfxar2o5O2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create the Sequential model.\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Determine the \"input shape\", which is the number\n",
    "# of features that we will feed into the model.\n",
    "input_shape = len(feature_columns)\n",
    "\n",
    "# Create a layer that accepts our features and outputs\n",
    "# a single value, the predicted median home price.\n",
    "layer = layers.Dense(1, input_shape=[input_shape])\n",
    "\n",
    "# Add the layer to our model.\n",
    "model.add(layer)\n",
    "\n",
    "# Print out a model summary.\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 1)                 14        \n",
      "=================================================================\n",
      "Total params: 14\n",
      "Trainable params: 14\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vkkNFjzop3I2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Above we have basically recreated our linear regression from an earlier lab. We have all of our inputs directly mapping to a single output. We didn't choose an activation function, and the default activation function for a [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer is a linear function $f(x) = x$.\n",
    "\n",
    "Note that the way we built this model was pretty verbose. You typically see simple models like this built in a more compact manner:"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "sdu82rLhrFXM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential(layers=[\n",
    "    layers.Dense(1, input_shape=[len(feature_columns)])\n",
    "])\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1)                 14        \n",
      "=================================================================\n",
      "Total params: 14\n",
      "Trainable params: 14\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MTCo5CpVLV2g"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also notice that the layers are named `dense_1`, `dense_2`, etc.\n",
    "\n",
    "If you don't supply a name for a layer, TensorFlow will provide a name for you. In small models, this isn't a problem, but you might want to have a meaningful layer name in larger models.\n",
    "\n",
    "Even in simple models, is `dense_2` a good name for the first layer in a model?"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "pEPw3kIhMtiW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 3: Name Your Layers\n",
    "\n",
    "The default naming scheme for layers can start to become confusing, especially if you repeatedly run a cell block to iterate on your model design.\n",
    "\n",
    "In this exercise consult the [`Dense` documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) and find the argument that allows you to name your layer. Use that argument in the code below to name your layer 'the_only_layer'. Note that you might have to consult the documentation for the parent classes of `Dense`.\n",
    "\n",
    "Also, don't forget to answer the question below the code block!"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "25xCx2aCNLcc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Student Solution**"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "j3SrSWLvViFn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential(layers=[\n",
    "    layers.Dense(\n",
    "        1,\n",
    "        input_shape=[len(feature_columns)],\n",
    "        # Name your layer here\n",
    "        name = \"the_only_layer\"\n",
    "    )\n",
    "])\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_only_layer (Dense)       (None, 1)                 14        \n",
      "=================================================================\n",
      "Total params: 14\n",
      "Trainable params: 14\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjbI2IPsVKlo"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Which class did the parameter that you used originate from?\n",
    "\n",
    "> Layer"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "4E3jXNquVhCn"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "sdvLFUFQVsVf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Making a Deep Neural Network"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "PXiiJF3fWReI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Where neural networks really get powerful is when you add **hidden layers**. These hidden layers can find complex patterns in your data.\n",
    "\n",
    "Let's create a model with a few hidden layers. We'll add two layers with sixty-four nodes each."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "Ef2RnBnKLKjM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "feature_count = len(feature_columns)\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.Dense(64, input_shape=[feature_count]),\n",
    "  layers.Dense(64),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                896       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,121\n",
      "Trainable params: 5,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eIz2AyIPr6Qt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now have a deep neural network model. The model has 13 input nodes. These nodes feed into our first hidden layer of 64 nodes.\n",
    "\n",
    "The first line of our model summary tells us that we have 64 nodes and 896 parameters. The node count in 'Output Shape' makes sense, but what about the 'Param #' of 896?\n",
    "\n",
    "Remember that we have 13 input nodes feeding into 64 nodes in our first hidden layer. The layers are densely connected, so each of the 13 input nodes connects to each of the 64 nodes in the next layer. `13 * 64 = 832` connections. Add another 64 for the number of nodes in the layer, and you get the 896 number.\n",
    "\n",
    "This pattern repeats for the next layer. 64 nodes connecting to 64 nodes: `64 * 64 + 64 = 4160`.\n",
    "\n",
    "And finally 64 nodes connect to the final output node: `64 * 1 + 1 = 65`.\n",
    "\n",
    "This makes for a total of 5121 parameters in the model. Even a very small neural network like this can have a lot of trainable parameters inside of it!"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "4BYvMgqWtblx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we start training it, we need to tell TensorFlow how and what to optimize the model for using the [`compile` method](https://keras.io/models/model/#compile). In our example below, we are optimizing for mean squared error using the [Adam optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam).  We'll calculate and report the mean squared error and mean absolute error along the way."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "ktQvZT_jS0UY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "model.compile(\n",
    "  loss='mse',\n",
    "  optimizer='Adam',\n",
    "  metrics=['mae', 'mse'],\n",
    "\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                896       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,121\n",
      "Trainable params: 5,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58EnP9hwuCxe"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the Model"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "vwa9xoVwulvh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now train the model using the [`fit()` method](https://keras.io/models/model/#fit). Training is performed for a specified number of **epochs**. An epoch is a full pass over the training data. In this case, we are asking to train over the full dataset 50 times.\n",
    "\n",
    "In order to get the data into the model, we don't have to write an input function like we did with the `Estimator` API. The Keras API provides for a much more direct format."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "e-8Vaoryu9rv"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "model.fit(\n",
    "  training_df[feature_columns],\n",
    "  training_df[target_column],\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=0.2,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-26 12:58:58.755732: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 1.1158 - mae: 0.7424 - mse: 1.1158 - val_loss: 0.5905 - val_mae: 0.5570 - val_mse: 0.5905\n",
      "Epoch 2/50\n",
      "413/413 [==============================] - 0s 589us/step - loss: 0.5161 - mae: 0.5184 - mse: 0.5161 - val_loss: 0.4746 - val_mae: 0.5148 - val_mse: 0.4746\n",
      "Epoch 3/50\n",
      "413/413 [==============================] - 0s 582us/step - loss: 0.5256 - mae: 0.5239 - mse: 0.5256 - val_loss: 0.5054 - val_mae: 0.5419 - val_mse: 0.5054\n",
      "Epoch 4/50\n",
      "413/413 [==============================] - 0s 588us/step - loss: 0.5063 - mae: 0.5136 - mse: 0.5063 - val_loss: 0.4900 - val_mae: 0.5236 - val_mse: 0.4900\n",
      "Epoch 5/50\n",
      "413/413 [==============================] - 0s 573us/step - loss: 0.5116 - mae: 0.5153 - mse: 0.5116 - val_loss: 0.4651 - val_mae: 0.5030 - val_mse: 0.4651\n",
      "Epoch 6/50\n",
      "413/413 [==============================] - 0s 568us/step - loss: 0.5212 - mae: 0.5178 - mse: 0.5212 - val_loss: 0.4751 - val_mae: 0.4957 - val_mse: 0.4751\n",
      "Epoch 7/50\n",
      "413/413 [==============================] - 0s 570us/step - loss: 0.5183 - mae: 0.5158 - mse: 0.5183 - val_loss: 0.4699 - val_mae: 0.5087 - val_mse: 0.4699\n",
      "Epoch 8/50\n",
      "413/413 [==============================] - 0s 580us/step - loss: 0.5030 - mae: 0.5116 - mse: 0.5030 - val_loss: 0.4836 - val_mae: 0.5216 - val_mse: 0.4836\n",
      "Epoch 9/50\n",
      "413/413 [==============================] - 0s 634us/step - loss: 0.4877 - mae: 0.5114 - mse: 0.4877 - val_loss: 0.4820 - val_mae: 0.4976 - val_mse: 0.4820\n",
      "Epoch 10/50\n",
      "413/413 [==============================] - 0s 645us/step - loss: 0.5009 - mae: 0.5104 - mse: 0.5009 - val_loss: 0.4708 - val_mae: 0.5015 - val_mse: 0.4708\n",
      "Epoch 11/50\n",
      "413/413 [==============================] - 0s 654us/step - loss: 0.4936 - mae: 0.5046 - mse: 0.4936 - val_loss: 0.4637 - val_mae: 0.4998 - val_mse: 0.4637\n",
      "Epoch 12/50\n",
      "413/413 [==============================] - 0s 642us/step - loss: 0.4978 - mae: 0.5073 - mse: 0.4978 - val_loss: 0.4671 - val_mae: 0.5036 - val_mse: 0.4671\n",
      "Epoch 13/50\n",
      "413/413 [==============================] - 0s 642us/step - loss: 0.5007 - mae: 0.5118 - mse: 0.5007 - val_loss: 0.4796 - val_mae: 0.5245 - val_mse: 0.4796\n",
      "Epoch 14/50\n",
      "413/413 [==============================] - 0s 745us/step - loss: 0.4963 - mae: 0.5093 - mse: 0.4963 - val_loss: 0.4712 - val_mae: 0.5080 - val_mse: 0.4712\n",
      "Epoch 15/50\n",
      "413/413 [==============================] - 0s 630us/step - loss: 0.4917 - mae: 0.5074 - mse: 0.4917 - val_loss: 0.4639 - val_mae: 0.4966 - val_mse: 0.4639\n",
      "Epoch 16/50\n",
      "413/413 [==============================] - 0s 619us/step - loss: 0.4858 - mae: 0.5045 - mse: 0.4858 - val_loss: 0.5131 - val_mae: 0.5190 - val_mse: 0.5131\n",
      "Epoch 17/50\n",
      "413/413 [==============================] - 0s 687us/step - loss: 0.5051 - mae: 0.5119 - mse: 0.5051 - val_loss: 0.4697 - val_mae: 0.5063 - val_mse: 0.4697\n",
      "Epoch 18/50\n",
      "413/413 [==============================] - 0s 631us/step - loss: 0.4718 - mae: 0.4979 - mse: 0.4718 - val_loss: 0.4768 - val_mae: 0.5197 - val_mse: 0.4768\n",
      "Epoch 19/50\n",
      "413/413 [==============================] - 0s 595us/step - loss: 0.4866 - mae: 0.5084 - mse: 0.4866 - val_loss: 0.4763 - val_mae: 0.5073 - val_mse: 0.4763\n",
      "Epoch 20/50\n",
      "413/413 [==============================] - 0s 668us/step - loss: 0.4933 - mae: 0.5057 - mse: 0.4933 - val_loss: 0.4709 - val_mae: 0.4958 - val_mse: 0.4709\n",
      "Epoch 21/50\n",
      "413/413 [==============================] - 0s 576us/step - loss: 0.5153 - mae: 0.5148 - mse: 0.5153 - val_loss: 0.4692 - val_mae: 0.5021 - val_mse: 0.4692\n",
      "Epoch 22/50\n",
      "413/413 [==============================] - 0s 568us/step - loss: 0.4881 - mae: 0.5038 - mse: 0.4881 - val_loss: 0.4683 - val_mae: 0.5079 - val_mse: 0.4683\n",
      "Epoch 23/50\n",
      "413/413 [==============================] - 0s 587us/step - loss: 0.4942 - mae: 0.5070 - mse: 0.4942 - val_loss: 0.4731 - val_mae: 0.5013 - val_mse: 0.4731\n",
      "Epoch 24/50\n",
      "413/413 [==============================] - 0s 569us/step - loss: 0.4749 - mae: 0.4998 - mse: 0.4749 - val_loss: 0.4810 - val_mae: 0.5030 - val_mse: 0.4810\n",
      "Epoch 25/50\n",
      "413/413 [==============================] - 0s 614us/step - loss: 0.4850 - mae: 0.5038 - mse: 0.4850 - val_loss: 0.4746 - val_mae: 0.5112 - val_mse: 0.4746\n",
      "Epoch 26/50\n",
      "413/413 [==============================] - 0s 576us/step - loss: 0.5193 - mae: 0.5168 - mse: 0.5193 - val_loss: 0.4726 - val_mae: 0.4985 - val_mse: 0.4726\n",
      "Epoch 27/50\n",
      "413/413 [==============================] - 0s 569us/step - loss: 0.5036 - mae: 0.5102 - mse: 0.5036 - val_loss: 0.4645 - val_mae: 0.4993 - val_mse: 0.4645\n",
      "Epoch 28/50\n",
      "413/413 [==============================] - 0s 566us/step - loss: 0.4946 - mae: 0.5091 - mse: 0.4946 - val_loss: 0.4726 - val_mae: 0.5047 - val_mse: 0.4726\n",
      "Epoch 29/50\n",
      "413/413 [==============================] - 0s 629us/step - loss: 0.4963 - mae: 0.5070 - mse: 0.4963 - val_loss: 0.4653 - val_mae: 0.4989 - val_mse: 0.4653\n",
      "Epoch 30/50\n",
      "413/413 [==============================] - 0s 578us/step - loss: 0.4783 - mae: 0.4978 - mse: 0.4783 - val_loss: 0.4737 - val_mae: 0.5176 - val_mse: 0.4737\n",
      "Epoch 31/50\n",
      "413/413 [==============================] - 0s 632us/step - loss: 0.4817 - mae: 0.5037 - mse: 0.4817 - val_loss: 0.4723 - val_mae: 0.5149 - val_mse: 0.4723\n",
      "Epoch 32/50\n",
      "413/413 [==============================] - 0s 617us/step - loss: 0.4755 - mae: 0.5046 - mse: 0.4755 - val_loss: 0.4730 - val_mae: 0.4938 - val_mse: 0.4730\n",
      "Epoch 33/50\n",
      "413/413 [==============================] - 0s 566us/step - loss: 0.4812 - mae: 0.4948 - mse: 0.4812 - val_loss: 0.4649 - val_mae: 0.5056 - val_mse: 0.4649\n",
      "Epoch 34/50\n",
      "413/413 [==============================] - 0s 668us/step - loss: 0.4897 - mae: 0.5034 - mse: 0.4897 - val_loss: 0.4666 - val_mae: 0.4968 - val_mse: 0.4666\n",
      "Epoch 35/50\n",
      "413/413 [==============================] - 0s 577us/step - loss: 0.4894 - mae: 0.5023 - mse: 0.4894 - val_loss: 0.4688 - val_mae: 0.4962 - val_mse: 0.4688\n",
      "Epoch 36/50\n",
      "413/413 [==============================] - 0s 563us/step - loss: 0.4727 - mae: 0.4979 - mse: 0.4727 - val_loss: 0.4627 - val_mae: 0.5028 - val_mse: 0.4627\n",
      "Epoch 37/50\n",
      "413/413 [==============================] - 0s 549us/step - loss: 0.4794 - mae: 0.5013 - mse: 0.4794 - val_loss: 0.5152 - val_mae: 0.5612 - val_mse: 0.5152\n",
      "Epoch 38/50\n",
      "413/413 [==============================] - 0s 572us/step - loss: 0.4905 - mae: 0.5025 - mse: 0.4905 - val_loss: 0.4661 - val_mae: 0.4985 - val_mse: 0.4661\n",
      "Epoch 39/50\n",
      "413/413 [==============================] - 0s 621us/step - loss: 0.4969 - mae: 0.5084 - mse: 0.4969 - val_loss: 0.4649 - val_mae: 0.5055 - val_mse: 0.4649\n",
      "Epoch 40/50\n",
      "413/413 [==============================] - 0s 621us/step - loss: 0.4936 - mae: 0.5043 - mse: 0.4936 - val_loss: 0.4658 - val_mae: 0.5054 - val_mse: 0.4658\n",
      "Epoch 41/50\n",
      "413/413 [==============================] - 0s 583us/step - loss: 0.4952 - mae: 0.5087 - mse: 0.4952 - val_loss: 0.4679 - val_mae: 0.4961 - val_mse: 0.4679\n",
      "Epoch 42/50\n",
      "413/413 [==============================] - 0s 566us/step - loss: 0.4933 - mae: 0.5034 - mse: 0.4933 - val_loss: 0.4640 - val_mae: 0.5032 - val_mse: 0.4640\n",
      "Epoch 43/50\n",
      "413/413 [==============================] - 0s 594us/step - loss: 0.4716 - mae: 0.4973 - mse: 0.4716 - val_loss: 0.4684 - val_mae: 0.5077 - val_mse: 0.4684\n",
      "Epoch 44/50\n",
      "413/413 [==============================] - 0s 622us/step - loss: 0.4925 - mae: 0.5061 - mse: 0.4925 - val_loss: 0.4694 - val_mae: 0.5049 - val_mse: 0.4694\n",
      "Epoch 45/50\n",
      "413/413 [==============================] - 0s 598us/step - loss: 0.4656 - mae: 0.4941 - mse: 0.4656 - val_loss: 0.4675 - val_mae: 0.4999 - val_mse: 0.4675\n",
      "Epoch 46/50\n",
      "413/413 [==============================] - 0s 631us/step - loss: 0.4882 - mae: 0.5049 - mse: 0.4882 - val_loss: 0.4685 - val_mae: 0.5134 - val_mse: 0.4685\n",
      "Epoch 47/50\n",
      "413/413 [==============================] - 0s 598us/step - loss: 0.4845 - mae: 0.4997 - mse: 0.4845 - val_loss: 0.4783 - val_mae: 0.5005 - val_mse: 0.4783\n",
      "Epoch 48/50\n",
      "413/413 [==============================] - 0s 620us/step - loss: 0.4900 - mae: 0.5076 - mse: 0.4900 - val_loss: 0.4649 - val_mae: 0.5009 - val_mse: 0.4649\n",
      "Epoch 49/50\n",
      "413/413 [==============================] - 0s 580us/step - loss: 0.4904 - mae: 0.5042 - mse: 0.4904 - val_loss: 0.4770 - val_mae: 0.4960 - val_mse: 0.4770\n",
      "Epoch 50/50\n",
      "413/413 [==============================] - 0s 604us/step - loss: 0.4816 - mae: 0.4980 - mse: 0.4816 - val_loss: 0.4659 - val_mae: 0.5000 - val_mse: 0.4659\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa82d286130>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "suWnhYbUuoYC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validating the Model"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "TuTsElUdv4uU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now see how well our model performs on our validation test set. In order to get the model to make predictions, we use the [`predict` method](https://keras.io/models/model/#predict)."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "pFLnpozdwYbL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "predictions = model.predict(testing_df[feature_columns])\n",
    "\n",
    "predictions"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.7994311 ],\n",
       "       [5.2114167 ],\n",
       "       [0.67390215],\n",
       "       ...,\n",
       "       [1.182355  ],\n",
       "       [1.6297004 ],\n",
       "       [1.2653005 ]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qzC4Q6kuxy-m"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that the predictions are lists of lists. This is because neural networks can return more than one prediction per input. We set this network up to have a single final node, but could have had more."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "3nAGc2VQyIGu"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 4: Calculating RMSE\n",
    "\n",
    "At this point we have the predicted values from our test features and the actual values. In this exercise you are tasked with computing the root-mean squared error of those predictions. Given the predictions stored in `predictions` above, write code that computes the root mean squared error of those predictions vs. the truth found in `testing_df`. Print the root mean squared error."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "XH_6-meyzBIj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Student Solution**"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "GuY3HZAZl4z7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import math\n",
    "from sklearn import metrics\n",
    "root_mean_squared_error = math.sqrt(\n",
    "    metrics.mean_squared_error(\n",
    "      predictions * TARGET_FACTOR,\n",
    "      testing_df[target_column] * TARGET_FACTOR\n",
    "))\n",
    "print(root_mean_squared_error)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "67086.37054826478\n"
     ]
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_flON0FHc_yW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "4DncNYnRdBQ6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Improving the Model"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "33SMPRXPeLmv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the exercise above, you likely got a root mean squared error very close to the error we got in the linear regression lab. What's going on? I thought deep learning models were supposed to be really, really good!\n",
    "\n",
    "Deep learning models can be really good, but they often require a bit of hyperparameter tuning. Aside from the breadth and depth of the hidden layers, the activation function for the model can have a big impact on how a model performs.\n",
    "\n",
    "Earlier we mentioned that the default activation function for `Dense` layers is the linear function $f(x) = x$. It turns out that if you stack layers of linear functions, you just get a single linear function, so the network that we built is basically just one big linear regression.\n",
    "\n",
    "We can change the activation function layer by layer for our model. In order to do that, we just pass an `activation` argument to our `Dense` class. Keras has [many built-in activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations) that you can reference by name like:\n",
    "\n",
    "```python\n",
    "  layers.Dense(64, activation='sigmoid')\n",
    "```\n",
    "\n",
    "For activations that aren't built into Keras, you can use the full path to their class:\n",
    "\n",
    "```python\n",
    "  layers.Dense(64, activation=tf.nn.swish)\n",
    "```\n",
    "\n",
    "The [`tf.nn` namespace](https://www.tensorflow.org/api_docs/python/tf/nn) is a little crowded, but there are activations functions in there including [`swish`](https://www.tensorflow.org/api_docs/python/tf/nn/swish), [`leaky_relu`](https://www.tensorflow.org/api_docs/python/tf/nn/leaky_relu), and more."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "YAtfBRAXeSYW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 5: A Better Activation Function\n",
    "\n",
    "Experiment with different activation functions and find one that performs better than the linear activation that we used above. You can set the activation function on any or all of the layers in the network. The functions don't have to be the same.\n",
    "\n",
    "Print out the root mean squared error once you find an acceptable activation function."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "M-BNO2fAeOZ2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Student Solution**"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "cMgsYuE0ld9g"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "feature_count = len(feature_columns)\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.Dense(64, input_shape=[feature_count], activation='relu'),\n",
    "  layers.Dense(64, activation='relu'),\n",
    "  layers.Dense(1, activation='relu')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "  loss='mse',\n",
    "  optimizer='Adam',\n",
    "  metrics=['mae', 'mse'],\n",
    ")\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "model.fit(\n",
    "  training_df[feature_columns],\n",
    "  training_df[target_column],\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=0.2,\n",
    ")\n",
    "\n",
    "predictions = model.predict(testing_df[feature_columns])\n",
    "\n",
    "mean_squared_error = metrics.mean_squared_error(\n",
    "    (predictions) * TARGET_FACTOR,\n",
    "    testing_df[target_column] * TARGET_FACTOR\n",
    ")\n",
    "\n",
    "print(\"Mean Squared Error (on training data): %0.3f\" % mean_squared_error)\n",
    "\n",
    "root_mean_squared_error = math.sqrt(mean_squared_error)\n",
    "print(\"Root Mean Squared Error (on training data): %0.3f\" % root_mean_squared_error)\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "413/413 [==============================] - 1s 856us/step - loss: 1.5169 - mae: 0.8464 - mse: 1.5169 - val_loss: 0.4302 - val_mae: 0.4932 - val_mse: 0.4302\n",
      "Epoch 2/50\n",
      "413/413 [==============================] - 0s 622us/step - loss: 0.4033 - mae: 0.4478 - mse: 0.4033 - val_loss: 0.3593 - val_mae: 0.4247 - val_mse: 0.3593\n",
      "Epoch 3/50\n",
      "413/413 [==============================] - 0s 595us/step - loss: 0.3728 - mae: 0.4272 - mse: 0.3728 - val_loss: 0.3363 - val_mae: 0.4179 - val_mse: 0.3363\n",
      "Epoch 4/50\n",
      "413/413 [==============================] - 0s 652us/step - loss: 0.3736 - mae: 0.4241 - mse: 0.3736 - val_loss: 0.3264 - val_mae: 0.4009 - val_mse: 0.3264\n",
      "Epoch 5/50\n",
      "413/413 [==============================] - 0s 673us/step - loss: 0.3461 - mae: 0.4048 - mse: 0.3461 - val_loss: 0.3231 - val_mae: 0.3969 - val_mse: 0.3231\n",
      "Epoch 6/50\n",
      "413/413 [==============================] - 0s 579us/step - loss: 0.3417 - mae: 0.4032 - mse: 0.3417 - val_loss: 0.3176 - val_mae: 0.4020 - val_mse: 0.3176\n",
      "Epoch 7/50\n",
      "413/413 [==============================] - 0s 671us/step - loss: 0.3184 - mae: 0.3924 - mse: 0.3184 - val_loss: 0.3132 - val_mae: 0.3870 - val_mse: 0.3132\n",
      "Epoch 8/50\n",
      "413/413 [==============================] - 0s 636us/step - loss: 0.3282 - mae: 0.3949 - mse: 0.3282 - val_loss: 0.3070 - val_mae: 0.3897 - val_mse: 0.3070\n",
      "Epoch 9/50\n",
      "413/413 [==============================] - 0s 577us/step - loss: 0.3115 - mae: 0.3825 - mse: 0.3115 - val_loss: 0.3138 - val_mae: 0.3874 - val_mse: 0.3138\n",
      "Epoch 10/50\n",
      "413/413 [==============================] - 0s 579us/step - loss: 0.3194 - mae: 0.3905 - mse: 0.3194 - val_loss: 0.3184 - val_mae: 0.4147 - val_mse: 0.3184\n",
      "Epoch 11/50\n",
      "413/413 [==============================] - 0s 669us/step - loss: 0.3166 - mae: 0.3869 - mse: 0.3166 - val_loss: 0.2959 - val_mae: 0.3747 - val_mse: 0.2959\n",
      "Epoch 12/50\n",
      "413/413 [==============================] - 0s 593us/step - loss: 0.3105 - mae: 0.3804 - mse: 0.3105 - val_loss: 0.3022 - val_mae: 0.3809 - val_mse: 0.3022\n",
      "Epoch 13/50\n",
      "413/413 [==============================] - 0s 578us/step - loss: 0.3092 - mae: 0.3809 - mse: 0.3092 - val_loss: 0.2925 - val_mae: 0.3763 - val_mse: 0.2925\n",
      "Epoch 14/50\n",
      "413/413 [==============================] - 0s 578us/step - loss: 0.2993 - mae: 0.3746 - mse: 0.2993 - val_loss: 0.2935 - val_mae: 0.3858 - val_mse: 0.2935\n",
      "Epoch 15/50\n",
      "413/413 [==============================] - 0s 579us/step - loss: 0.2895 - mae: 0.3677 - mse: 0.2895 - val_loss: 0.2911 - val_mae: 0.3741 - val_mse: 0.2911\n",
      "Epoch 16/50\n",
      "413/413 [==============================] - 0s 574us/step - loss: 0.2999 - mae: 0.3740 - mse: 0.2999 - val_loss: 0.2908 - val_mae: 0.3724 - val_mse: 0.2908\n",
      "Epoch 17/50\n",
      "413/413 [==============================] - 0s 572us/step - loss: 0.2932 - mae: 0.3698 - mse: 0.2932 - val_loss: 0.2886 - val_mae: 0.3675 - val_mse: 0.2886\n",
      "Epoch 18/50\n",
      "413/413 [==============================] - 0s 573us/step - loss: 0.2802 - mae: 0.3639 - mse: 0.2802 - val_loss: 0.2860 - val_mae: 0.3746 - val_mse: 0.2860\n",
      "Epoch 19/50\n",
      "413/413 [==============================] - 0s 627us/step - loss: 0.2819 - mae: 0.3633 - mse: 0.2819 - val_loss: 0.2933 - val_mae: 0.3919 - val_mse: 0.2933\n",
      "Epoch 20/50\n",
      "413/413 [==============================] - 0s 583us/step - loss: 0.2781 - mae: 0.3619 - mse: 0.2781 - val_loss: 0.2840 - val_mae: 0.3776 - val_mse: 0.2840\n",
      "Epoch 21/50\n",
      "413/413 [==============================] - 0s 606us/step - loss: 0.2733 - mae: 0.3573 - mse: 0.2733 - val_loss: 0.2780 - val_mae: 0.3646 - val_mse: 0.2780\n",
      "Epoch 22/50\n",
      "413/413 [==============================] - 0s 578us/step - loss: 0.2707 - mae: 0.3553 - mse: 0.2707 - val_loss: 0.3051 - val_mae: 0.3766 - val_mse: 0.3051\n",
      "Epoch 23/50\n",
      "413/413 [==============================] - 0s 570us/step - loss: 0.2632 - mae: 0.3516 - mse: 0.2632 - val_loss: 0.2849 - val_mae: 0.3682 - val_mse: 0.2849\n",
      "Epoch 24/50\n",
      "413/413 [==============================] - 0s 570us/step - loss: 0.2612 - mae: 0.3513 - mse: 0.2612 - val_loss: 0.3178 - val_mae: 0.3998 - val_mse: 0.3178\n",
      "Epoch 25/50\n",
      "413/413 [==============================] - 0s 570us/step - loss: 0.2676 - mae: 0.3569 - mse: 0.2676 - val_loss: 0.2799 - val_mae: 0.3601 - val_mse: 0.2799\n",
      "Epoch 26/50\n",
      "413/413 [==============================] - 0s 570us/step - loss: 0.2749 - mae: 0.3588 - mse: 0.2749 - val_loss: 0.2734 - val_mae: 0.3573 - val_mse: 0.2734\n",
      "Epoch 27/50\n",
      "413/413 [==============================] - 0s 570us/step - loss: 0.2742 - mae: 0.3570 - mse: 0.2742 - val_loss: 0.2780 - val_mae: 0.3675 - val_mse: 0.2780\n",
      "Epoch 28/50\n",
      "413/413 [==============================] - 0s 573us/step - loss: 0.2671 - mae: 0.3497 - mse: 0.2671 - val_loss: 0.2968 - val_mae: 0.3972 - val_mse: 0.2968\n",
      "Epoch 29/50\n",
      "413/413 [==============================] - 0s 588us/step - loss: 0.2679 - mae: 0.3528 - mse: 0.2679 - val_loss: 0.2701 - val_mae: 0.3562 - val_mse: 0.2701\n",
      "Epoch 30/50\n",
      "413/413 [==============================] - 0s 572us/step - loss: 0.2537 - mae: 0.3443 - mse: 0.2537 - val_loss: 0.2704 - val_mae: 0.3592 - val_mse: 0.2704\n",
      "Epoch 31/50\n",
      "413/413 [==============================] - 0s 573us/step - loss: 0.2563 - mae: 0.3466 - mse: 0.2563 - val_loss: 0.2734 - val_mae: 0.3567 - val_mse: 0.2734\n",
      "Epoch 32/50\n",
      "413/413 [==============================] - 0s 573us/step - loss: 0.2594 - mae: 0.3463 - mse: 0.2594 - val_loss: 0.2727 - val_mae: 0.3539 - val_mse: 0.2727\n",
      "Epoch 33/50\n",
      "413/413 [==============================] - 0s 698us/step - loss: 0.2547 - mae: 0.3463 - mse: 0.2547 - val_loss: 0.2848 - val_mae: 0.3819 - val_mse: 0.2848\n",
      "Epoch 34/50\n",
      "413/413 [==============================] - 0s 565us/step - loss: 0.2549 - mae: 0.3457 - mse: 0.2549 - val_loss: 0.2735 - val_mae: 0.3663 - val_mse: 0.2735\n",
      "Epoch 35/50\n",
      "413/413 [==============================] - 0s 568us/step - loss: 0.2506 - mae: 0.3413 - mse: 0.2506 - val_loss: 0.2692 - val_mae: 0.3495 - val_mse: 0.2692\n",
      "Epoch 36/50\n",
      "413/413 [==============================] - 0s 623us/step - loss: 0.2472 - mae: 0.3417 - mse: 0.2472 - val_loss: 0.2966 - val_mae: 0.3953 - val_mse: 0.2966\n",
      "Epoch 37/50\n",
      "413/413 [==============================] - 0s 568us/step - loss: 0.2503 - mae: 0.3445 - mse: 0.2503 - val_loss: 0.2812 - val_mae: 0.3693 - val_mse: 0.2812\n",
      "Epoch 38/50\n",
      "413/413 [==============================] - 0s 584us/step - loss: 0.2466 - mae: 0.3383 - mse: 0.2466 - val_loss: 0.2863 - val_mae: 0.3640 - val_mse: 0.2863\n",
      "Epoch 39/50\n",
      "413/413 [==============================] - 0s 572us/step - loss: 0.2504 - mae: 0.3453 - mse: 0.2504 - val_loss: 0.2769 - val_mae: 0.3532 - val_mse: 0.2769\n",
      "Epoch 40/50\n",
      "413/413 [==============================] - 0s 567us/step - loss: 0.2530 - mae: 0.3432 - mse: 0.2530 - val_loss: 0.2749 - val_mae: 0.3516 - val_mse: 0.2749\n",
      "Epoch 41/50\n",
      "413/413 [==============================] - 0s 577us/step - loss: 0.2523 - mae: 0.3400 - mse: 0.2523 - val_loss: 0.2658 - val_mae: 0.3516 - val_mse: 0.2658\n",
      "Epoch 42/50\n",
      "413/413 [==============================] - 0s 586us/step - loss: 0.2502 - mae: 0.3410 - mse: 0.2502 - val_loss: 0.2731 - val_mae: 0.3515 - val_mse: 0.2731\n",
      "Epoch 43/50\n",
      "413/413 [==============================] - 0s 566us/step - loss: 0.2450 - mae: 0.3367 - mse: 0.2450 - val_loss: 0.2810 - val_mae: 0.3706 - val_mse: 0.2810\n",
      "Epoch 44/50\n",
      "413/413 [==============================] - 0s 572us/step - loss: 0.2469 - mae: 0.3373 - mse: 0.2469 - val_loss: 0.2720 - val_mae: 0.3649 - val_mse: 0.2720\n",
      "Epoch 45/50\n",
      "413/413 [==============================] - 0s 568us/step - loss: 0.2372 - mae: 0.3314 - mse: 0.2372 - val_loss: 0.2689 - val_mae: 0.3533 - val_mse: 0.2689\n",
      "Epoch 46/50\n",
      "413/413 [==============================] - 0s 566us/step - loss: 0.2382 - mae: 0.3342 - mse: 0.2382 - val_loss: 0.2723 - val_mae: 0.3500 - val_mse: 0.2723\n",
      "Epoch 47/50\n",
      "413/413 [==============================] - 0s 570us/step - loss: 0.2446 - mae: 0.3381 - mse: 0.2446 - val_loss: 0.2692 - val_mae: 0.3508 - val_mse: 0.2692\n",
      "Epoch 48/50\n",
      "413/413 [==============================] - 0s 570us/step - loss: 0.2426 - mae: 0.3355 - mse: 0.2426 - val_loss: 0.2711 - val_mae: 0.3593 - val_mse: 0.2711\n",
      "Epoch 49/50\n",
      "413/413 [==============================] - 0s 576us/step - loss: 0.2472 - mae: 0.3378 - mse: 0.2472 - val_loss: 0.3079 - val_mae: 0.3684 - val_mse: 0.3079\n",
      "Epoch 50/50\n",
      "413/413 [==============================] - 0s 581us/step - loss: 0.2414 - mae: 0.3351 - mse: 0.2414 - val_loss: 0.2823 - val_mae: 0.3660 - val_mse: 0.2823\n",
      "Mean Squared Error (on training data): 2835015765.907\n",
      "Root Mean Squared Error (on training data): 53244.866\n"
     ]
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HuWQBoAslfo0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "J2csiDfLlgtT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing Training"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "7E63WyKpmleN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "At this point, we have a pretty solid neural network regression model. It performs better than our linear regression model, though it does take a while to train.\n",
    "\n",
    "Training time is largely a product of two factors:\n",
    "\n",
    "1. The size of the model\n",
    "1. The number of epochs\n",
    "\n",
    "Larger models take longer to train. That shouldn't come as a surprise. Remember from above that we calculated the number of parameters in our model. Every layer that is densely connected adds many more parameters that need to be adjusted during training.\n",
    "\n",
    "Our goal is to find a model that is big enough, but not too big. This, it turns out, is very much an area where experimentation is required.\n",
    "\n",
    "The second determination of model training time is the number of epochs. We can choose an arbitrary number of epochs from one to infinity. How many do we need?\n",
    "\n",
    "It turns out that we can be much more scientific about this parameter. As a model begins to converge, there is less and less benefit for each subsequent epoch.\n",
    "\n",
    "More training does not necessarily mean a better model.\n",
    "\n",
    "There are a few ways to determine the appropriate number of epochs. One is to plot the error and see when it flattens out.\n",
    "\n",
    "It turns out that our model actually returns the error values when you fit the model."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "ixm_bQDGuYbr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "model = keras.Sequential([\n",
    "  layers.Dense(64, input_shape=[feature_count]),\n",
    "  layers.Dense(64),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "  loss='mse',\n",
    "  optimizer='Adam',\n",
    "  metrics=['mae', 'mse'],\n",
    ")\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "history = model.fit(\n",
    "  training_df[feature_columns],\n",
    "  training_df[target_column],\n",
    "  epochs=EPOCHS,\n",
    "  verbose=0,                     # New parameter to make model training silent\n",
    "  validation_split=0.2,\n",
    ")\n",
    "\n",
    "history.history"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'loss': [0.6914031505584717,\n",
       "  0.5216115117073059,\n",
       "  0.5207009315490723,\n",
       "  0.5094817876815796,\n",
       "  0.5087965130805969],\n",
       " 'mae': [0.5852765440940857,\n",
       "  0.5183658003807068,\n",
       "  0.5199006795883179,\n",
       "  0.5155811905860901,\n",
       "  0.5126842856407166],\n",
       " 'mse': [0.6914031505584717,\n",
       "  0.5216115117073059,\n",
       "  0.5207009315490723,\n",
       "  0.5094817876815796,\n",
       "  0.5087965130805969],\n",
       " 'val_loss': [0.5172968506813049,\n",
       "  0.49058935046195984,\n",
       "  0.4676525890827179,\n",
       "  0.48629119992256165,\n",
       "  0.4728599190711975],\n",
       " 'val_mae': [0.5433453917503357,\n",
       "  0.5126517415046692,\n",
       "  0.5102482438087463,\n",
       "  0.5018937587738037,\n",
       "  0.5010738372802734],\n",
       " 'val_mse': [0.5172968506813049,\n",
       "  0.49058935046195984,\n",
       "  0.4676525890827179,\n",
       "  0.48629119992256165,\n",
       "  0.4728599190711975]}"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KbhsGp9_0LOK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that the `history.history` contains our model's loss (`loss`), mean absolute error (`mae`), mean squared error (`mse`), validation loss (`val_loss`), validation mean absolute error (`val_mae`), and validation mean squared error (`val_mse`) at each epoch.\n",
    "\n",
    "It would be useful to plot the error over time. In the next exercise, you will create a visualization that will help us determine when to stop training the model."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "VqUttCc121Mo"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 6: Plotting Error"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "8_Pn1ZFe3mEp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use `matplotlib.pyplot` or `seaborn` to create a line plot that shows the mean squared error and the validation mean squared error per epoch.\n",
    "\n",
    "In the code block below, we save the errors per epoch in the variable `history`. Inspect the variable and plot a line plot which has the epoch on the x-axis and the mean squared error on the y-axis. There should be two lines on the visualization: mean absolute error and validation mean absolute error.\n",
    "\n",
    "Note that we created the model with the default activation function. Use the activation function that you found to be more useful in exercise 5.\n",
    "\n",
    "The result should be a line plot of epoch and error with two lines similar to:\n",
    "\n",
    "![alt text](https://i.imgur.com/0YDzVhq.png)"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "HERuu5h332oV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "model = keras.Sequential([\n",
    "  layers.Dense(64, input_shape=[feature_count]),\n",
    "  layers.Dense(64),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "  loss='mse',\n",
    "  optimizer='Adam',\n",
    "  metrics=['mae', 'mse'],\n",
    ")\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "history = model.fit(\n",
    "  training_df[feature_columns],\n",
    "  training_df[target_column],\n",
    "  epochs=EPOCHS,\n",
    "  verbose=0,\n",
    "  validation_split=0.2,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qK0v5ehj58nx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Student Solution**"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "H743wjkE56AV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "history_dict = history.history\n",
    "\n",
    "#['val_mse]\n",
    "#['mse']\n",
    "\n",
    "\n",
    "y_1 = history_dict['val_mse']\n",
    "y_2 = history_dict['mse']\n",
    "\n",
    "\n",
    "plt.plot(y_1)\n",
    "plt.plot(y_2)\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA7MUlEQVR4nO3dd3gc1dn38e/ZXfUuS7JlSbbcey/Yxja9dwLEkFBCqIE3CXkIoSQ8gbQnIUBCSRwglNCbAeMYbLBxw1XuttwkF0mWZDWr19097x9nJe9qV/bKljCM78916bJ3dmZ3zmr1mzP3nJlRWmuEEEJYl+1kr4AQQojuJUEvhBAWJ0EvhBAWJ0EvhBAWJ0EvhBAW5zjZKxBIUlKSzszMPNmrIYQQ3xnr168v01onB3ruWxn0mZmZZGVlnezVEEKI7wyl1IGOnpPSjRBCWJwEvRBCWJwEvRBCWJwEvRBCWJwEvRBCWJwEvRBCWJwEvRBCWJy1gn7pXyDny5O9FkII8a1iraBf8TTkfnWy10IIIb5VrBX0thBwO0/2WgghxLeKtYLe7gBXy8leCyGE+FaxVtDbQsAtQS+EEN6sFfT2EHBJ6UYIIbwFFfRKqQuVUruUUjlKqQcDPP9LpdQmz882pZRLKZUYzLJdyuaQHr0QQrRzzKBXStmB54GLgOHA9Uqp4d7zaK2f0FqP1VqPBR4ClmqtK4JZtkvZQ6RGL4QQ7QTTo58M5Git92qtm4F3gCuOMv/1wNvHueyJkVE3QgjhJ5igTwPyvR4XeKb5UUpFAhcCHx7HsncopbKUUlmlpaVBrFYAMupGCCH8BBP0KsA03cG8lwFfa60rOrus1voFrfVErfXE5OSAd8M6NqnRCyGEn2CCvgDI8HqcDhR2MO8sjpRtOrvsibNJjV4IIdoLJujXAYOUUv2UUqGYMJ/bfialVBxwBvBJZ5ftMvYQcLu67eWFEOK76Jg3B9daO5VS9wILADvwstZ6u1LqLs/zsz2zXgUs1FrXHWvZrm5EG5sDnI3d9vJCCPFddMygB9Bazwfmt5s2u93jV4FXg1m228jwSiGE8GOtM2NleKUQQvixVtDL8EohhPBjraCXi5oJIYQfawW9XNRMCCH8WCvo5YQpIYTwY62gl1E3Qgjhx1pBLzV6IYTwY62glxq9EEL4sVbQS41eCCH8WCvopUYvhBB+rBX0thBAy4XNhBDCi7WC3u65dI/06oUQoo21gt4WYv6VOr0QQrSxVtDbPUEvPXohhGhjraC3eUo3cgVLIYRoY82glx69EEK0sVbQ26VGL4QQ7Vkr6FsPxsrZsUII0cZaQW+XGr0QQrRnraCX4ZVCCOHHWkEvwyuFEMKPtYK+rUcvpRshhGhlraCXSyAIIYQfawW91OiFEMKPtYLeLsMrhRCiPWsFfdslEKRHL4QQrawV9DLqRggh/Fgr6KVGL4QQfqwV9FKjF0IIP9YKeqnRCyGEH2sFvdTohRDCT1BBr5S6UCm1SymVo5R6sIN5zlRKbVJKbVdKLfWavl8ptdXzXFZXrXhAcmasEEL4cRxrBqWUHXgeOA8oANYppeZqrbO95okH/gFcqLXOU0qltHuZs7TWZV232h2QM2OFEMJPMD36yUCO1nqv1roZeAe4ot08NwBztNZ5AFrrkq5dzSDJqBshhPATTNCnAflejws807wNBhKUUkuUUuuVUjd5PaeBhZ7pd3T0JkqpO5RSWUqprNLS0mDX35fU6IUQws8xSzeACjBNB3idCcA5QASwSim1Wmu9Gzhda13oKed8oZTaqbVe5veCWr8AvAAwceLE9q8fHLk5uBBC+AmmR18AZHg9TgcKA8zzuda6zlOLXwaMAdBaF3r+LQE+wpSCuofNDijp0QshhJdggn4dMEgp1U8pFQrMAua2m+cTYIZSyqGUigROA3YopaKUUjEASqko4HxgW9etfgD2EKnRCyGEl2OWbrTWTqXUvcACwA68rLXerpS6y/P8bK31DqXU58AWwA28pLXeppTqD3yklGp9r7e01p93V2MAc0DW7erWtxBCiO+SYGr0aK3nA/PbTZvd7vETwBPtpu3FU8L5xtgdUroRQggv1jozFjw9egl6IYRoZb2gt4dIj14IIbxYL+htITK8UgghvFgv6KVGL4QQPqwX9FKjF0IIH9YLenuI3HhECCG8WC/obQ7p0QshhBfrBb2MuhFCCB/WC3oZdSOEED6sF/Qy6kYIIXxYL+hl1I0QQviwXtBLjV4IIXxYL+htDqnRCyGEF+sFvfTohRDCh/WCXmr0Qgjhw4JB75AzY4UQwov1gt4uZ8YKIYQ36wW9TWr0QgjhzXpBb5czY4UQwpv1gl6GVwohhA/rBb0MrxRCCB/WC/rW4ZVan+w1EUKIbwXrBb09xPzrdp3c9RBCiG8J6wW9zWH+lSGWQggBWDHoW3v0UqcXQgjAikFvay3dyMgbIYQAKwa93VO6kR69EEIAVgz6th69BL0QQoAVg15q9EII4cN6QS81eiGE8GG9oJcavRBC+Agq6JVSFyqldimlcpRSD3Ywz5lKqU1Kqe1KqaWdWbZLSY1eCCF8OI41g1LKDjwPnAcUAOuUUnO11tle88QD/wAu1FrnKaVSgl22y7XV6KV0I4QQEFyPfjKQo7Xeq7VuBt4Brmg3zw3AHK11HoDWuqQTy3YtOTNWCCF8BBP0aUC+1+MCzzRvg4EEpdQSpdR6pdRNnVgWAKXUHUqpLKVUVmlpaXBrH4iMuhFCCB/HLN0AKsC09peGdAATgHOACGCVUmp1kMuaiVq/ALwAMHHixOO/9KT06IUQwkcwQV8AZHg9TgcKA8xTprWuA+qUUsuAMUEu27VsUqMXQghvwZRu1gGDlFL9lFKhwCxgbrt5PgFmKKUcSqlI4DRgR5DLdi279OiFEMLbMXv0WmunUupeYAFgB17WWm9XSt3leX621nqHUupzYAvgBl7SWm8DCLRsN7XFsEmNXgghvAVTukFrPR+Y327a7HaPnwCeCGbZbmWXM2OFEMKb9c6MbTsYK0EvhBBgxaCX4ZVCCOHDekEvl0AQQggf1gt6uQSCEEL4sF7QywlTQgjhw3pBLzV6IYTwYb2glxq9EEL4sF7QS41eCCF8WC/olQJllx69EEJ4WC/owfTqpUYvhBCAVYPeFiJnxgohhIc1g97ukB69EEJ4WDPobSFSoxdCCA9rBr09REbdCCGEhzWD3uaQHr0QQnhYN+ilRi+EEIBVg94uNXohhGhlzaC3SY1eCCFaWTPo7VKjF0KIVtYMepucGSuEEK2sGfT2EHC7TvZaCCHEt4I1g16GVwohRBtrBr1c1EwIIdpYM+jlEghCCNHGUkE/7U+LePqL3Z6LmsnwSiGEAIsFfX2Li8r6ZunRCyGEF0sFfVSog9oml9TohRDCi7WCPsxOfbNTbjwihBBeLBX0kaEOapuccuMRIYTwYqmgjw5zUNfklBq9EEJ4sVTQm9KNS248IoQQXoIKeqXUhUqpXUqpHKXUgwGeP1MpVaWU2uT5edTruf1Kqa2e6VldufLtRbWWbuTMWCGEaOM41gxKKTvwPHAeUACsU0rN1Vpnt5t1udb60g5e5iytddmJreqxRbWWbmTUjRBCtAmmRz8ZyNFa79VaNwPvAFd072odn6gwB3XNLlOj1y7Q+mSvkhBCnHTBBH0akO/1uMAzrb2pSqnNSqnPlFIjvKZrYKFSar1S6o6O3kQpdYdSKksplVVaWhrUyrcXFWqn2enGpexmgvTqhRAiqKBXAaa17ypvAPpqrccAzwIfez13utZ6PHARcI9SamagN9Fav6C1nqi1npicnBzEavmLCjOVqGbtCXqp0wshRFBBXwBkeD1OBwq9Z9BaV2utaz3/nw+EKKWSPI8LPf+WAB9hSkHdIirMBHyz9jRLevRCCBFU0K8DBiml+imlQoFZwFzvGZRSvZRSyvP/yZ7XLVdKRSmlYjzTo4DzgW1d2QBvrT36JrenWXJ2rBBCHHvUjdbaqZS6F1gA2IGXtdbblVJ3eZ6fDVwD3K2UcgINwCyttVZK9QQ+8mwDHMBbWuvPu6ktbUHf6JYevRBCtDpm0ENbOWZ+u2mzvf7/HPBcgOX2AmNOcB2DFhXq6dG7pEYvhBCtLHdmLECD9OiFEKKNtYLe06NvdHkGCskNwoUQwmJB76nRN7haD8ZKj14IISwV9NHtg15KN0IIYa2gDw+xYVNQ31a6keGVQghhqaBXShEV6qDO6Ql66dELIYS1gh5Mnb6+RWr0QgjRynJBHxlmp7Z1sI306IUQwnpBHx3moK5FavRCCNHKckEfGWqntkVq9EII0cpyQR8d5qCmNd+lRi+EENYL+qgwh1ePXko3QghhuaCPDHVQ3ex5ID16IYSwXtBHh9mPBL3U6IUQwnpBHxnqoFZq9EII0cZyQR8d5sBJ683BpUYvhBCWC/qoMAfO1vupSI9eCCGsGPR2Wtp69BL0QghhvaAP9SrdyJmxQghhwaD3qdFLj14IISwY9HZA4VYOqdELIQSWDHpzINZtc0jpRgghsGLQe24Q7lYOGV4phBBYMejDTH1eSjdCCGFYLugjPT16F/ZjH4wtyILsT/wml9c2sSq3vDtWTwghvnGWC3q7TRERYqcqNAV2L4Ca4sAzOpvgvZvgvZshd7HPU5ve/xP6tctocboCLyuEEN8hlgt6MAdk3+/9K2iqNmHubPafadObUH0QopJgzh1Qc8hM3/oB5xx4mmm27VQW5X6zKy6EEN3AokFvZ689E654HvLXwGcP+M7gbIblT0HaRLhpLjTVwEd3wr5l8PHdHLL3AqA+b/M3v/JCCNHFrBn0oQ7qmpww8mqYfh+sfwVWPgtamxm2vANV+XDmg9BzOFz0Z9j7FfznSkjI5E7H73Brhbtoy0lthxBCdAVLBn10mIPaJs/QyrN/A0MvhYW/hg9/DPUVsOyv0HscDDzXzDP+Zhg9C6J74rr+PbbWRLNP9yKkdNvJa4QQQnQRx8lege4QGWanos5Tl7fZ4brX4eunYfEfIOdLaKyCi/4CynPLQaXgqtngdlJS68TlziZb9+WMyp0nrxFCCNFFLNmjj/Lu0QPYbDDjf+CW/0JIFKRPgsEX+C6kFNhDKKxsBCDbnUlsUyE0VH5zKy6EEN0gqKBXSl2olNqllMpRSj0Y4PkzlVJVSqlNnp9Hg122O0SHOqhvCjA0su9U+NkmuPnTI735doqqGgDYRV8z4dD2blpLIYT4ZhyzdKOUsgPPA+cBBcA6pdRcrXV2u1mXa60vPc5lu1RkmN0cjA3EEXbUZYs8PfqmpJFQBRRvhczTu3gNhRDimxNMjX4ykKO13guglHoHuAIIJqxPZNnjFh3moK7ZidYa1UHPvSOFVQ1EhdpJ6JlKRXU8icVbA8/YUAnFW6ClAVzNYA+DAWeD3ZKHPYQQ32HBpFIakO/1uAA4LcB8U5VSm4FC4H6t9fZOLItS6g7gDoA+ffoEsVodiwx14NbQ2OImItTeqWWLKhvpHR9BSkw4O9x9OL3Ya4hlYxUsfxL2LjUhr92+C4+/GS77e4dlISGEOBmCCfpAqaXbPd4A9NVa1yqlLgY+BgYFuayZqPULwAsAEydODDhPsKI9FzarbXJ2OugLqxpIjY8gOSaMra6+TCtdgHK1gD0EFj0OWS9Dn2kw8wHocxqEx4E9FLa8a8bq9xgIp//0RFb/W+e5xXsY3DOG80f0OtmrIoQ4DsEEfQGQ4fU4HdNrb6O1rvb6/3yl1D+UUknBLNsdWq9JX9fkJDnm6DX59gorGxmeGktKTBhL3X1RrmYo2w1hsbD+NRh/k+m1t5cyAirz4YtHIbEfDLvsyHNaw67PYM0/YfBFMOXu70yvX2vNP5fkMnVADwl6Ib6jggn6dcAgpVQ/4CAwC7jBewalVC/gkNZaK6UmY0bzlAOVx1q2O7RewbKuuXPXo29yuiirbSI1zvTot2vPyJvirZC3yoTzjPsDL2yzmbH4VQXw4e0w4ipIyITIRNjwmnmNsDhzmYWS7XDJ0+AIPfoK7V0KhRug93hIGw9hMZ1qT1eoqGumrtlFweGGb/y9hRBd45hBr7V2KqXuBRYAduBlrfV2pdRdnudnA9cAdyulnEADMEtrrYGAy3ZTW9pEt/XoO3f1yUNVTQCkxoeTHBPGPp2Kyx6OfcensPtzmPAjiM/o+AVCIuD6t+GTe2HfUtj8lpneYyBc9S8Y+T1Y+hdY9hco3wvnPQ6xqRDd05SGvG34D3z6syPHAZQNUoZD+kRzHkD6JOgxyGxgwOw1HN4PlQcgYwqEhHeq7R3Jq6gH4ODhhuM6uC2EOPmCGiKitZ4PzG83bbbX/58Dngt22e4W6anRd7ZHX+gZQ9/b06N3Y6MiaiDJO+eZUTUzfnHsF4lOgR+8Z/7f0gg1RRDfx5yhC3D2I5A8BD7+Cfz73CPLpU2ASbeZPYG1L8IXvzGXaLj8WTiUDQVrzfXzt30E6181y4TFQu+xEJUMeWugusAzPQ6GXwajvw+ZM06oTNQa9DVNTqobnMRFhhxjCSHEt40lxwJGe9XoO6P1ZKnU+HASIkOx2xSF4YNIrt4Gk34Msb07tyIh4aZe396oayDjNHMyVm0xVB00N0D5+G6Y/wA018CIq81egCPUvO8gz0bB7YbyPVCwDg5ugIProSwHMiZDv/sgpjfs+BS2fwIb3zBlnzMfgkHnmbLStg/M9fdTx8DwK80GpmIvbH0fdswzG6T4DIjrA/YQMvcf4g+OEppx0PTlGkhJhQHnQPJg3zYdyjblrYRM6DEA4jKObNw8vsjK5rXPVzD7/luIDj9G2UoI0WUsGfRRxxn0rZc/6B0Xgd2m6BEVytawcYyJ+hpO/3nXrmR8hm8Z6KyHYf8Kc6XN2N5w7mN+QQmYUk3yEEgeQvOoG3h83nayGg4z9+rphDo8ZZyhF8OlT8GW92D5X+Gta03wVnlGuiYPg9WzzSih8HhorAQU9J0GjnAo3QU5i8DtYrAOobfdRhhOYjcsMMuHRsMN7x05kWz/1/DmtdBSd2Q9HRHQa5TZ4wiLgb1LOOfgBs5D0/TMkzD2Whh1HaSO7trPtSvVlpjPonwPNFbDhJshIuFkr5UQnWbNoPcMqexsjb6wsoH4yJC2IZnJMWEstk/jh/f/tPtHySgF/WaYnyCU1zZx9xsbWLu/AoD95XUM7ul1sDYkwgTT2Btg8zuwfY4Z5z/qGrOX0VBpjjvkLjaBPOJqiEvze59bX1jNoepG9pbV8eglQ7h1hA3e+j688T2Y9SbYHPD2LIhLh2tfhYbDUJ4LJTugaBNsfBOcDei0ifxLXUtucwL3hGfTb/U/zYZmwDlmjyNjktkzWfcibP/YbOTC481GoqUOGqrMjWR6joCB58DA88wxi2BPUGs4DNVFZiPpvQF1Oc0Jb6GRR6ZVF8LnD/rfZnLtC2YvK9DvyNUCRZshsb85AN/2vpXmTmeRPcxyxzgz+5TiajHfk54jAndqRJexZtAH2aPfV1ZHZX0z4/qYXlpRVSO94yLank+OCaO0pulbNxQyp6SWm19eS1ltE/eeNZDnvsphV3GNb9C3sofA+BvNj7eIeBgzy/wcRV5FPZMyEyiubiS/sgkSR8At8+H1K03AK5sJt5s+MccnADKnH3kBtwucjWwrcfLn51YAcDj2Wv592wAzGmnls+ZYRfJQKN0JthCzRxIaY/Y0GqsgIs0MXw2JgINZ5nyGRY+b945Jhdg0E6Bup/nJOA3O+BWEx5p1yF0MH94G9eXmonZp482dxUp3m96622X2ZgZfYA5qL/2zeZ0Z95vpSYNN737O7fDaZTD1HnMw3B4KribY8yXs+q/ZmCg79JlqNkbFW2DnfDMPmDYNPAf6zYTUseZeCI5w08a6UjOM9+AGM9KqvsIce4lOMeWwgedA6rgjB98D0Rq9+h+4vvo/7JEJqMR+kDjADALoO61z32O3C7Z9aD7jYZcHHiHmcsKKp+DQNpj5S9NhCEZjtRlssPqf5rhS73Fw2TNH9u5cLVC0xbQ7qkfw69wZNYdMqdEeYr47UcnQa3Tgz6i53pRYGw6b72lo1PG/b2WeKa1WF5pOTGi0ec0BZx/9d3uCLBn0IXYboQ4bdc1H79H/5uNtbM6vZNXD5xAd5qCwsoH0BK+gjw5jZ1FNd69upz326Xbqm528d+dUhvSK4R9LcthzqOvXs8XlpqiqgT6JaaQnRHCwdYhldLK5MNxb3ze94R9+aIIzEJsdQqNYnpMDwJlDktmYX4mOSEBNvw8m3Q7rXoKd8+CsR3CNu5l3djTyvfHphId00MurKYbcr6Ai1xzfqC4w4WBzmKBd9Txs/QAu+AOU7THBnTwUzv2tGeaav9Ycr0ge4jn2oWDPF+aeBQCDzjeXsfY+vhKfAXcth88fglXtxh2ExcKQi8xyJTtg13xY9JjpxU+4BUZfZzYyO/9r9qKyPzbLKbtZ59YNQeu0lOEQ09OEf0m2CYWv/gBRKWYjGp9hjsXE9zHHZqKSoKkW5v4/1PY5rHKNZFjvfiQ1FZnyXda/TQhPvNXcK7loixni63abjWdIhNlwjboWUoZC4SaYd5/Z4ABE94LJt8GYG0xZUSk4fMBsPAvWmjJd9lyz/Gl3mmWaa83GMibVc2xLQe4i2PW5+QyaqqHvdDP/ymfhhTPN+jUcNr+LpiqzTNp4s9cX29tsdJQylx1prDYdAbfryPS4dDP4IGW4CU1XixmJ1lJvSpcRCeb3vvIZs6FxNvr+HnuPNzcqGnIRHFhpPrvdn5nfXdv3OcR85v3PNL/XhMzA39FWjVVmEEXBOnMuTdEmM90RAU6vIctJg2HKT0zHKyQi4EudCEsGPZjyzdF69I0tLtbtr6DJ6eaDrHxuOb0fRVWNTMo8studEhtGWW0TbrfGZvt29Op3H6ph+Z4y7j9/MGMy4gHITIpiVzcEfWFlA24NGYmRpCdE+o6lj0yEHy80PeAgeiJf55QxtFcM5w3vyZJdpeRXNNCnRySERcP0n5sfYNWeMh75aBuRoXauGpce+MViesHY6zt+s4PrYd4vzI1mAMZcD5c8efSe2HmPmd5WbakJl0A9u9AouPwZOOMBEzSuZjP8tecI35LMOb8xPcbIRN9hs617DJV5prdftMWETXSKCfGETBPI3mUkgLpyE5K7F5hg3TnPvHerpMHgakFXHuA59QOearmI/+kzlHvPHmR6o1vfM8dk5t1n5o9KgV4jzd5ES71py4qnzPGcpMFQnmM2Ule/CBGJsPp5WPx78xOVYnre+WvNa139ktlYfv138x5b3+v4MwbzusMuM4Mb0iaYaeNvNCcarnsRIpPM8wPOMiXAnC/MerW/3AiYsLSHmue0y7Sl9T0iEuHwPrOxafv9RXvCXZlAnXCL2dA6m+DQVlj5HLx3o/lcnI1mD2zoxeYziUk1PfCDWeb8lq/+CEv+ZPZ2TrvLbJR3/hf2LITmOjMQwx5mpqNp22id+xgMv9zsBbucZuDFni9h1bMw7+dm+PVPN3bZ8Oi2j6pLX+1bZEByNF9kH+JXFw1tG4XjbWNeJU1ON1Ghdl5ZuZ9rJmZQ1dBCavyRDzg5OgynW3O4vpke0d+O2uorX+8nzGHjhtP6tk0bnBLD7m4I+tahlX0SI0mLjyDLczygjVJBlQPMRvUwN07py7gMUybbmH/YBH072UVVAGw7WM1V445zxdMmwO2LYdNb5o921DXBlS3i+5ifY4lLh7hjzBPTM/B0pSChr/nxPnv6aKJ6mN7j6OvMY61NL7M8x5QfDqyCmiLeG/YMT25IJD4yhI15lWbe0EgTaONvNiWWqGSzoWyv5hBkf0zzto9R42cScu6vjxx4HnQulOw054YUbjK90vSJcOnTR3q05/7WBF7earNBDI0ClBleXF1oQrjfGWa59vX4iAQzjPic35qSovfzZ/7KbIia60yYazeERJq9qPblpMp82L/cnJTYVGM+36TBZl2qCswG1hFmhjG3Px+mz2nmPJnsT0ypb8BZMORi/9718MvNv1UHYe2/IOvVI3to4fFmry421Ww8WhpMWTFjkvlOhrf70tgdpu2jrzXf0f3LzUi8Lg55sHDQP3zJML73z5U8/cVufnPpcL/nV+0tx6bg15cO56E5W3lj9QGAdjV684GX1jb5Bb3T5Wb5njJmDk7G/g319ivqmpmzoYCrx6eRGHXkSz64VwwLs4tpbHF1XO44Dq1Bb3r0EVQ3OqlubCE2vHNj6bP2H6bZ6Wb6oCQG94wmMtTOxrxKrhjrf/A3u9BcTWN7YZXP9LomJ2f+dQm/vmRYwOX82Oz+xyWsQilTrolKgj5TYPp9FFU18OgTS7h8TC/CHDYW7SzxPcFNqaPX0GN64p50BzMXDeTcHin8vv3oopSh5udoYnrBiCuPv10d1ePDY48cbzma+Awz+GDscZ58b7Ob+0yPvPrY88almRMeZ/7S1Nzj0s2xmfYnPgZLKXPspt/M41v+GCx5hymA8X0SmDWpD6+u3N8WHt5W5ZYxKi2Oayek0zsunOe/MjXk1DivHr3nOjmlNU1+y8/fVsyPXl3Hnz//5m43+PbaPJqcbn50uu/Y/ME9o3FryC2t7dL3y6uoJ9Ruo2dsOOkJpvd98DguhbAip4wQu2JyZiIOu43R6XFszDsccN4dnmMi2wur0a03c8fsgZXWNPHZ1uLjaMl3V5PTdeS2mEfx5MLdaA2/vGAI4/smUFHXzIHy+k69V3ZRNcXVjazdV3HU+cprmzhYKZfEAEw5Z+wNJqCPN+S/AZYNeoBfXTiE+IgQfv3xVtzuI6FR3+xkU34lUwck4bDbuGlaJjWNppbXO9531A0EDvrlu0sBeGHZXt7Pyvd7vqs1O938Z9V+ZgxK8htd0/p4z6GuDfr8inrSE8w5Ba0HqY/nmjcrckoZ1yehbTTUuD4JbC+sprHF92B5Y4uLnNJaUmLCqGl0kl9x5L3WHzAbhtX7yn1+l1b3l893ceHfluE6Spt3Fdfw4YYCbp7Wl4zESMb1iQdgQwcb044s22O+03tKaqlpbAk4j9aaW19dx40vrenUa3vr7t/fs4v28Pin3XrLi+8cSwd9fGQoD108jA15lbzrFcZZ+w/T4tJMG2B2FWdNyiAixI5S0DP22D16rTUrcso4f3hPpg9M4pGPtvnXr7vYu1n5HKpu4tbp/mfaZvaIIsSuuvyAbH5FA+mJpief1hb0neslVtQ1s72wmukDj4zKGZcRj9Ot/cozOSW1uNyaq8ebg7Dez2cdMJ9vZX0LO4s7bud9727iyYW7OrWO31Zaaz7fVkxJTRM7ivz3SlvN2VCAw6b4yZkDARiUEkN0mONInT5Iy3eX4bAptIatB6sCzrN4ZwmbC6rYW1ZHfkXnvgtm+UOM/O2CLt/7bKW15j+rD/DmmgN+HYlTmaWDHuB749OY3C+RP3++s20XeGVuOSF2xcRMU4eMjwzlpql9Gdor9sjZpZiROxEhdr+g31tWR1FVI2cMSeb5G8aTlhDBXW+sp6Sm3XCtLpBdWM3NL6/lNx9vY3R6HGcMSvabJ9Rho19SVJcPscyrqKdPogn4HlGhhIfYfEo36w9UsHpveUeLA7AqtxytYfqgI0E/1tPjbB9ErSW2q8alYbcptnmC3uXWbMqr5OyhZpz+ytyygO91uK6Zjzcd5F/L9lJW678X9l2TW1rbViLp6HPWWvP59mKmDUgiwXPcxm5TjMmI61SPvr7ZyfoDh7l6vDn+sSm/MuB7/e3LPcRFmBLF1zmBfw8daWxx8b9zt1Pf7OKTTd1ztfLc0lpKa5pocrrJ2t+5PRors3zQK6X4/ZUjqWl08sQCU09flVvG2Iz4tssZA/zqwqH89/9N91s2OSaMknZB3/oFnzEwmbjIEGb/cAJltc3M7eIv7+yluVzy7HI25VfyyMXDeO/OqR0O8xzcM4bdXVi6qapvoaqhhT6eHr1SymeIpdaan7+7iZ+9s/Gou+LztxURHxnC6LQjIw5SYsJJT4jwD/qiaqJC7QxKiWZQSjTbPcG/+1ANNU1OLh2dSmaPyA5Db3lOGVqbMtfrqw6cSPPJr6jns61FfL6tiC+zD7GloPKYy3S1JbtMKSUhMoRVuYHbvLO4hgPl9Vw40nckzbiMBHYW11Af5IX91uyroNnl5rIxvcnsEcnmAEG/aEcJWw9W8cjFw0iJCWNFJ4P+3yv2kV/RQM/YMOZvLerUssFq/ZyUguU5pd3yHt9Flg96MCF46+mZvLMun2W7S9l6sIqpA3xP8LHZVMAQbTs71svyPWVkJEa0DQ8c0iuGob1iWLSjpMvWubHFxfNf5TB9YBLLHjiL22f2P+qImsE9Y8irqA/6D/tY8g8fGVrZKi0+goJKM31D3mHyKxo4VN3E5g5CsLy2iYXbi7l6XDoOu+9XbWxGvN8B2eyiaoamxmKzKUb0jmPbQRP0rfX5CX0TmDqgB2v2VuB0+Y+rXrqrlPjIEM4akszrqzu/6364rpnHP83m7CeXMOMvX3H3mxu4640N3PafLC5/7ms+2ljQqdc7UUt2lTIwJZoLR/Zi7b6KgHX6z7cVoxScN9x3OOf4vvG43JqtBYFLMO0t311GmMPGpMxExmTE+/Xotdb8bdFu+iRGctX4NE4fmMSq3OCPlxyqbuT5r3I4f3hP7jlrIDkltd1ykt/K3HLS4iOYnJnI8t3BbYgaW1ys3lvuc/Dfak6JoAf42bmDSYkJ4543N+DWtNXnjyUlJoxSrzKA0+VmdW65T80Z4OyhKazbX0FVQ+CDWE1OF2+tyWNfWV3A59tbsL2YmkYnd50xoG1X+WgG94wGTJ27K7QOrWwdbWP+H9HWo/9o40HCQ2w4bIoF2w8FfI05Gw7S4tJcP9n/Gv7j+iRQWNXYVufVWrOjqJphqebA8ojesZTVNlFS3cj6A4dJig6jT2IkUwckUdPkbOvtt3K7NUt3lzJjUDJ3zBzgGYp6sO35xhYXWwoq+WpnCe9l5QfcK/jrwl28tmo/GQmRPHrpcD69dzrzfzqDT++dzvg+8fx2bna3lOcCqW92snZfBWcOTmZK/x6eNvuH9oLtxUzKTCSp3fDfsZ7zFTYEWadfkVPK5H6JhIfYGZMez6HqJoqrjrT1yx0lbDtYzf87eyAhdhunD0yivK65w+MlOSW1/N9nO1m04xB1TU7+/NlOnC7NI5cM48IRvVAK5nfxCCq3W7NqbzlTB/Rg5uBksouqgyrh/e3LPcx6YTXvHcegihaXm4fmbOlwFNm3xSkT9NFhDn59yXBqmpyEOWxtIxOOpX2PfnNBFTVNTk5vF/TnDOuJ061Zttt/d3FlbhkX/X05D3+0lRteNBcJ87aruMZv2gfrC0iLj2Bq/+A2SK0jb3Yd5UBlq9omZ8AesbfWAPY+qSk9IZLK+hYq65uZt6WI84f3YuqAHizYXuzXG9Ja8866PCb0TWBQgGvwnD+8JzYFb63NA8xonppGJ8NTTYlnpKfUs62wivUHDjOhbzxKKab0N2cur2oX1K1/1GcMTmZK/0RGpcXx0oq9uN2a5XtKOfuvS7j8ua/50avreOCDLdzyyloq648MW2xyuvh0cyGXj+nNa7dO5tbp/RiVHsfw3rGMSo/jL9eMoaHFxW8+3haw51fd2MLdb6zngQ8289aaPLLbDQ/trFW55TS73Jw5JIUpnu9A+43TvrI6dhbXcGGAWzwmRoWS2SMyqAAqrmpk96FaZniOo7QeQ2nt1WuteWbRHvr2iOSqcaaGf/pAs06Bjpc0OV3c8+YGZi/N5cevZTH28YXM2XiQ22b0o2+PKFJiw5nUN7HLyzc7iquprG9h2oAebR2xYx1HaHa6+WB9PkrBo59sP+pB70A+2nCQt9fm88yiPR3Oo7Xmztez+OX7m0/aiLFTJugBLh2dyrnDenLe8J6EOYI7sSg5Ooyqhpa2IP46pwylYFq70s/YjHgSo0JZvPNI+cbl1jzwwWZueHENLS43v7tiBNUNLdz66jrqmpxorZm9NJeL/r6M619c3VZqKKxsYEVOGd+bkB70pRf69ogi1GFjzzF69HM2FDDmsYUMe/RzzntqKfe8tYED5f57GXkV9cRHhvicHNU6xPLNNXlU1rdw1bg0zh/Ri31ldX7vm3XgMLmldXx/UuA7cmUkRnLBiF68tSaPhmZX2x9Ya4++9d8lu0rJq6hnYl8T8Ckx4QxKifarWS/1bGBnDk5CKcVtM/qxt7SOm15ey43/XktEqJ3nbhjHRz+Zxmu3Tqaxxc37WUdKMYt2lFDd6Gw7GNnewJRofnHeYBZsP8S8Lb4BpbXmoTlbWZh9iC+yD/HwR1u5+Jnl/F8nz7Hw3jAs2VVKZKidSf0S6BkbTv+kKFbv9R3ZtWC76RFfMDLAma6Yc0k25FUec4PTWmuf4TnQPzw1lhC7agv6pZ5y5z1nDmwrwaXGRdA/OSpgnf6ZRXvYdaiG2T8cz1u3ncaPp/fn8jG9+clZA9vmuWhUL3YdqmnbA91ZXM01/1x5QqPXWr8TUwf0YGRaHHERISzfc/SgX5hdTFltM09cM4bYiBDueXMDtUFe3rzF5eaZxXtQynxGhR2cW/B1TjkLth/i/fUFbefrBLJoxyGeXLiLlmN0wo7HKRX0SilevGkCz90wPuhlzhnWk8hQO9//1yoOegJ4RO9YnzNTwYx0OGtICl/tKmnrLb+7Lp/3sgq4bXo/Fv78DG6cmslzPxjPzuIa7nlrA3e+vp7/+2wnEzMT2Vtax9Nf7gZMGGsN107o4FovAdhtigHJ0Ue9FMKnmwu5//3NTOybwG0z+pOZFMWSnSU88MEWvzAwI258L1HQOsTy3yv2kRgVyvRBSZzvqQ0v2Oa7G/7O2nyiwxxcOjq1w/W5dXo/qhpamLOxgOyiamwKhvYyZ0DGhIfQLymKD9ebMB7f98iZmlMH9GDd/gqfP4ilu0sZ0TuWFM/ZzBePSiUtPoKVuWXcObM///3pDC4d3ZtxfRI4Y3AykzITeGPNgbYe1pwNBfSMDfPbgHu7bXo/xmTE879zt/vUl99am8d/txRx//lD2PCb81j6yzO5enwaLyzb2+FB1Fa1TU5eW7mfs59cwhXPf01Rlbll45LdJUwb0KOtQzJlQA/W7vM9NvHZtmJGp8eRFh8R8LXH9YmnrLbpmOc+LN9TSlJ0GEN7mY1reIidYamxbM43G4nnFufQOy6cK8f5bgRPH5DE2n0VNDuPrNOm/Er+uSSXayekc+HIVKYNTOLBi4byzPXjfC5FctFI8734bGsRew7V8IMX15B14DD3v7/Z59hKUVUDZ/11CYN//RljHlvI1D8t4uUV+wK2Y1VuOf2Sokj13E9i+sAkVuwpO+qG7u21eaTFR3DVuDSevX4c+8vreHjO1qD2xj5YX0DB4QYev3wEbo1Px8HbM4v30Cs2nMvG9OapL3ezeKd/qbOirplffbiVL3eU0B2HCk6poAc6fc/T4b1jeeO20yiva+a62avYmHfYr2zT6pxhKVTWt7Axv5LK+maeWLCTyf0SeeSSYW3XuD9rSAqPXzGCJbtKWbyzhN9cOpx375jCrEkZvLhsLxvzDvPB+gKm9E8kI9H/WjBHM6RnNLs7KN18vq2Yn7+7iYl9E3nlR5P41YVDefGmiTx8yTDW7Kvw66XmV9STkeD7/q09+oq6Zi4bnUqI56zZ8X3iWZB9JOirGlr479ZCLh/b22dkU3sT+yYwKi2Ol1fsY3thNZlJUW2fE5jPvq7ZRajDxsi0I6fAT+3fg/pmV9tImOrGFjYcOMwZg48MPQ2x23jt1sn896czeOjiYX4Hsm+cmsmB8nqW7imlrLaJJbtKudIzrLMjDruNJ68djdaaS55ZwbOL9rC1oIrHPs1m5uBk7pzZH6UUfXtE8fsrR9I3MZL739/c4clHLyzLZeofF/G/c7cTE+Zgb2kdVz7/NXM3F5Jf0cAZQ1La5p3Svwe1XscmCisb2JxfyQUByjbeywD8bl52h73EkppGz7GNJJ+/jTHp8WwpqGTV3nKyDhzmzjMG+Aw9Bjh9YBL1za62nn9ji4v/eW8TvWLD+c1l/pcd8dYrLpwJfRN4f30B17+4BptN8cerRrG/vJ6/e8ogzU43P3lzAyXVjfxoWiZXjUsjIzGSx+dl81m7so/T5WbNvgqmeh17mz4oieLqxg6PW+0vq+PrnHJmTcrAblNM6d+DX5w3mLmbC/nt3O1HLbM0OV08tziHcX3i+eGUvkwfmMR7Wfl+B8xX7y1n7b4K7jqjP3/53miGp8bys7c3sdfrPAKtNY98tJXqhhaeum6M3+fcFU65oD8e4/sk8PbtU2hocdHi0n4HYlvNGJREiF3x5Y5DPPXFbqoaWvjtZSP8Ni4/OK0vf581lvfvmsqPp/dDKcXDlwyjZ2w4t72Wxf7yeq6dcJSbkHdgUM8YCqsauXb2Su55cwMPfriFm19ey9lPLuGetzYwOj2Ol380ySd8Z03qw4jesfxx/o62ETvPLtrD/vJ6Rqf7XoQpOTqMMM+X0Lt3d8GIXmw7WE1+RT0ut+afS3JpbHFz/aSjXyBMKcWPp/cjt7SOr3aWMDzV93omI3ub9x+dFudTapvSvwdKwVNf7KaoqoGVOWU43don6MGUW4alBr5GyoUjepEcE8Z/Vu7n082FON2aqzu6WqbPa8bwxS/O4LwRPXnyi91c8fwK4iNCeOq6MT5ltshQB09eN5aiqgZ+N8//LM2PNx7kj/N3MjEzgY9+Mo1P7p3OB3dPxWGz8bN3NgFwpld7pvQzpavVe8s5VN3IAx9sMe3ooGwD5vvw2OUjWJh9iF+8t9kvhGqbnPzolXU0tbj5cbsT8cZmxFPX7OKRj7aRFB0WsAQ3tX8PbAqW7S5l3pZCrpm9ktzSOv58zeigrod08ahUz0F/zdu3T+GG0/pw3cR0Xli2l+zCan43L5uNeZU8ce0YHrp4GL+9fAT/uXUy4/rE84v3NrPN66SubYXV1DY5fQZZtP6dLt9TRk5JLX+cv4OH5mxtO8j89ro87DbFdV5tu+esgdwxsz+vrTrA/7y/ucMN5HtZBRysbOAX5w1GKcX3J2VwsLLB75jAs4v3kBwTxqzJfYgItfOvGycQ4rBx3b9Wt5XePtlUyGfbivnF+YM7/L6eKMte1KyrjUyL4707pzB/a3FbT6m9mPAQTuvXgzkbDlJe28QPp/RleO/Av7j2F+aKDQ/hT1eP4pZX1hEVaueiUR3/AXfk8jG9ySmppbCygR1F1VQ3ttArLpwhPWO4eGQqt8/s73clT7tN8djlI7hm9ir+8VUuIXYbT3+5m6vHpXHbjP4+8yqlSEuIwOXWjPVcIhlM0P/ps508/1UO2wqr2HawmotG9mJUuw1FIBePSuWP83dQUtPk9yUf4fnsJmT6XmArISqUxy8fwR/m7+C8p5bRJzGS6DCHT3nnWEIdNq6f3IdnF+8hp7SWkWmxDOkV4MYtASRFh/H8DeO5fEwxs5fm8tBFw/xGvYAZDnr3mQN4/qtcRqfH84PT+qCUYtvBKh6cs4XJ/RJ54aaJhHjq3kN7xfLRPdO4+40NaK199uhSYsMZkBzFu1n5zF6aS0OLiz9eNYoBydFHXdebp2XS2OLiT5/tJNRu449XjyTMYafZ6eau19ezs7iGl26a2Hbwu1XrJbD3ldXx4EVDAw7tjYsMYVRaHM956s79kqJ4+vtj2mr9x3L1uDR2FFVzx8z+DEwx7Xj44mEs3lnCLa+spaSmiTtm9ufiUUfKf+Ehdl64cSJXPLeC217L4uVbJpGRGNEWsN5/mxmJkWT2iOSvC3fx+LxsHJ4h1PO2FPLwxcP4IKuAc4am+JwNr5TioYuGEhcRwhMLdlFR18xp/RNpcWpaXG5qm5zUNjn5amcJE/smtG1Mzh/Rk/jIEN5dl89MzwZ6/YEKvs4p59eXHNmjTE+I5O3bp3Dfu5u48/X1XDIqlWV7SpnYN4Hb2/29dSX1bRw7OnHiRJ2VlXWyV+O4vLxiH4/PyyYhMoSv7j+T+MjO3QT76S92ExPu8AvZ7nbfu5v4ZNNB3Bq+Nz6dv1wzOmAZY8muEiJDHUzul+gz/cK/LWNncQ09Y8P49SXDuXR0atBlsucW7+GvC3fzyi2TOGvokXKF6XGu5dFLRwTcaBwor+ORj7axIqeMC0b05F83TuxUm4urGjn9z4txuTWPXjo84OUlTlSz081NL69h9d4KJvRN4BfnDeaBD7bg1pq5905vu8xGey639vv8H/loK2+uyWNUWhxPf39sWzgG4+9f7uHpL3cTYlcMS40lxG5j/YHDPHHNaK6d6N9bd7s1Yx5fiE0pvn7w7ICX+gb4cH0Bn2wu5Ien9eHcYT275L4N87YUcu9bG5nSP5E3fnya3zkYYM6ivmb2Suq9bi40pGcMC+7zvfrjv5bm8tHGg1w1Lo2rx6dT1+TkgQ+3tF247dUfTeJMrxKZt9dX7efxedm0uExG2pS5e11MmIPE6FD+dNVon+/l459m8/rq/Xx8z+lsL6zmla/3U1LdyPJfneVXwmxxufnHV7k8u3gPoQ4bn/1sBn17nMCdqwCl1HqtdcA/Agn6LlZwuJ6zn1zK768Y6bNL+G1XUt3Ixc8s57zhPfn9laM6fenlZbtL2ZxfyY+m9+swFDpS1+Tk7bV53Dwts613GyytNUt2lTK4V0yHByWP5p43N7BgezGrHz4nYK+8K7jdmg/WF/Dnz3dSXtdMqMPG+3dObes1B6u4qpFlu82xhM7WcbU25xms3lvBpvzD5JTUcefM/tw+s+MOxWsr95MYFcplY3p36r1OVOu6juuTcNRzSPaV1bEx7zDltc2U1TYxY1Cyz6U2OuJ2a95cc4Dsomr+cOWoo26cWg8Mh9htx/yb2FVcwwV/W9b2OCk6jEcvG87lR/n8ckpqaWxx+e1RHQ8J+m9YQ7PL56Did4XT5Q7Ye7Kystom9pfVMTEz8dgzn6CqhhZeWr6X0enxfmeyCmt4afnetms7De0V0+nBHydCgl4IISzuaEF/anXfhBDiFCRBL4QQFidBL4QQFidBL4QQFidBL4QQFidBL4QQFidBL4QQFidBL4QQFvetPGFKKVUKHO/dnZOAzt21+LvvVGwznJrtPhXbDKdmuzvb5r5a64BXlPtWBv2JUEpldXR2mFWdim2GU7Pdp2Kb4dRsd1e2WUo3QghhcRL0QghhcVYM+hdO9gqcBKdim+HUbPep2GY4NdvdZW22XI1eCCGELyv26IUQQniRoBdCCIuzTNArpS5USu1SSuUopR482evTXZRSGUqpr5RSO5RS25VSP/NMT1RKfaGU2uP5N/g7ZX9HKKXsSqmNSql5nsenQpvjlVIfKKV2en7nU63ebqXUfZ7v9jal1NtKqXArtlkp9bJSqkQptc1rWoftVEo95Mm3XUqpCzrzXpYIeqWUHXgeuAgYDlyvlBp+cteq2ziB/9FaDwOmAPd42vogsEhrPQhY5HlsNT8Ddng9PhXa/Hfgc631UGAMpv2WbbdSKg34KTBRaz0SsAOzsGabXwUubDctYDs9f+OzgBGeZf7hyb2gWCLogclAjtZ6r9a6GXgHuOIkr1O30FoXaa03eP5fg/nDT8O09zXPbK8BV56UFewmSql04BLgJa/JVm9zLDAT+DeA1rpZa12JxdsNOIAIpZQDiAQKsWCbtdbLgIp2kztq5xXAO1rrJq31PiAHk3tBsUrQpwH5Xo8LPNMsTSmVCYwD1gA9tdZFYDYGQMpJXLXu8DfgAcDtNc3qbe4PlAKveEpWLymlorBwu7XWB4G/AnlAEVCltV6IhdvcTkftPKGMs0rQB7rVuqXHjSqlooEPgZ9rratP9vp0J6XUpUCJ1nr9yV6Xb5gDGA/8U2s9DqjDGiWLDnlq0lcA/YDeQJRS6ocnd62+FU4o46wS9AVAhtfjdMzuniUppUIwIf+m1nqOZ/IhpVSq5/lUoORkrV83OB24XCm1H1OWO1sp9QbWbjOY73WB1nqN5/EHmOC3crvPBfZprUu11i3AHGAa1m6zt47aeUIZZ5WgXwcMUkr1U0qFYg5azD3J69QtlFIKU7PdobV+yuupucDNnv/fDHzyTa9bd9FaP6S1TtdaZ2J+t4u11j/Ewm0G0FoXA/lKqSGeSecA2Vi73XnAFKVUpOe7fg7mOJSV2+yto3bOBWYppcKUUv2AQcDaoF9Va22JH+BiYDeQCzxystenG9s5HbPLtgXY5Pm5GOiBOUq/x/Nv4sle125q/5nAPM//Ld9mYCyQ5fl9fwwkWL3dwGPATmAb8DoQZsU2A29jjkO0YHrsPz5aO4FHPPm2C7ioM+8ll0AQQgiLs0rpRgghRAck6IUQwuIk6IUQwuIk6IUQwuIk6IUQwuIk6IUQwuIk6IUQwuL+P/ErMLma9Lm8AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rN-BXBX6KkV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "q6XBdKqj6L7Z"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Interpreting Loss Visualizations"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "xqp93VIH7eEJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have now created a visualization that should look something like this:\n",
    "\n",
    "![alt text](https://i.imgur.com/0YDzVhq.png)\n",
    "\n",
    "But how do we interpret this visualization?\n",
    "\n",
    "The blue line is the mean squared error for the training data. You can see it plummeting fast as the model quickly learns.\n",
    "\n",
    "The orange line is the validation data. This is a holdout set of data that the model checks after each epoch. You can see it dropping pretty quickly, too, but then it seems to stabilize somewhat by 20 epochs.\n",
    "\n",
    "Toward the right side of the graph, you can see that our validation set says volatile but relatively flat, while our training data set keeps getting better and better.\n",
    "\n",
    "Should we train more or less?\n",
    "\n",
    "The constantly reducing blue line is actually a signal of overfitting on the training data.\n",
    "\n",
    "The flat(ish) orange line signals this model is as good as we can get.\n",
    "\n",
    "For this model we could possibly stop training after even 25 epochs and get similar performance.\n",
    "\n",
    "But how do you know when to stop?\n",
    "\n",
    "Luckily there is an *early stopping* algorithm that allows a model to stop training when validation data isn't improving.\n",
    "\n",
    "In the example below, we set up a model to train for 1000 epochs; however, we add an early stopping callback. Early stopping stops training when the model isn't progressing upon validation.\n",
    "\n",
    "If you run the code block below, you'll see far fewer than 1000 epochs run."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "_B4Cwtbs7ifl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "source": [
    "model = keras.Sequential([\n",
    "  layers.Dense(64, input_shape=[feature_count]),\n",
    "  layers.Dense(64),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "  loss='mse',\n",
    "  optimizer='Adam',\n",
    "  metrics=['mae', 'mse'],\n",
    ")\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = model.fit(\n",
    "  training_df[feature_columns],\n",
    "  training_df[target_column],\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=0.2,\n",
    "  callbacks=[early_stop],\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 13209 samples, validate on 3303 samples\n",
      "Epoch 1/1000\n",
      "13209/13209 [==============================] - 1s 88us/sample - loss: 0.6024 - mae: 0.5471 - mse: 0.6024 - val_loss: 0.5527 - val_mae: 0.5278 - val_mse: 0.5527\n",
      "Epoch 2/1000\n",
      "13209/13209 [==============================] - 1s 57us/sample - loss: 0.4962 - mae: 0.5127 - mse: 0.4962 - val_loss: 0.5246 - val_mae: 0.5167 - val_mse: 0.5246\n",
      "Epoch 3/1000\n",
      "13209/13209 [==============================] - 1s 62us/sample - loss: 0.4906 - mae: 0.5118 - mse: 0.4906 - val_loss: 0.5353 - val_mae: 0.5074 - val_mse: 0.5353\n",
      "Epoch 4/1000\n",
      "13209/13209 [==============================] - 1s 58us/sample - loss: 0.4874 - mae: 0.5092 - mse: 0.4874 - val_loss: 0.5266 - val_mae: 0.5176 - val_mse: 0.5266\n",
      "Epoch 5/1000\n",
      "13209/13209 [==============================] - 1s 59us/sample - loss: 0.4828 - mae: 0.5053 - mse: 0.4828 - val_loss: 0.5374 - val_mae: 0.5182 - val_mse: 0.5374\n",
      "Epoch 6/1000\n",
      "13209/13209 [==============================] - 1s 59us/sample - loss: 0.4834 - mae: 0.5068 - mse: 0.4834 - val_loss: 0.5229 - val_mae: 0.5254 - val_mse: 0.5229\n",
      "Epoch 7/1000\n",
      "13209/13209 [==============================] - 1s 58us/sample - loss: 0.4861 - mae: 0.5080 - mse: 0.4861 - val_loss: 0.5379 - val_mae: 0.5336 - val_mse: 0.5379\n",
      "Epoch 8/1000\n",
      "13209/13209 [==============================] - 1s 58us/sample - loss: 0.4824 - mae: 0.5051 - mse: 0.4824 - val_loss: 0.5282 - val_mae: 0.5150 - val_mse: 0.5282\n",
      "Epoch 9/1000\n",
      "13209/13209 [==============================] - 1s 63us/sample - loss: 0.4802 - mae: 0.5048 - mse: 0.4802 - val_loss: 0.5209 - val_mae: 0.5084 - val_mse: 0.5209\n",
      "Epoch 10/1000\n",
      "13209/13209 [==============================] - 1s 63us/sample - loss: 0.4794 - mae: 0.5033 - mse: 0.4794 - val_loss: 0.5417 - val_mae: 0.5265 - val_mse: 0.5417\n",
      "Epoch 11/1000\n",
      "13209/13209 [==============================] - 1s 61us/sample - loss: 0.4783 - mae: 0.5023 - mse: 0.4783 - val_loss: 0.5341 - val_mae: 0.5218 - val_mse: 0.5341\n",
      "Epoch 12/1000\n",
      "13209/13209 [==============================] - 1s 59us/sample - loss: 0.4777 - mae: 0.5028 - mse: 0.4777 - val_loss: 0.5248 - val_mae: 0.5103 - val_mse: 0.5248\n",
      "Epoch 13/1000\n",
      "13209/13209 [==============================] - 1s 62us/sample - loss: 0.4763 - mae: 0.5022 - mse: 0.4763 - val_loss: 0.5259 - val_mae: 0.5232 - val_mse: 0.5259\n",
      "Epoch 14/1000\n",
      "13209/13209 [==============================] - 1s 57us/sample - loss: 0.4829 - mae: 0.5053 - mse: 0.4829 - val_loss: 0.5291 - val_mae: 0.5183 - val_mse: 0.5291\n",
      "Epoch 15/1000\n",
      "13209/13209 [==============================] - 1s 57us/sample - loss: 0.4765 - mae: 0.5020 - mse: 0.4765 - val_loss: 0.5296 - val_mae: 0.5287 - val_mse: 0.5296\n",
      "Epoch 16/1000\n",
      "13209/13209 [==============================] - 1s 57us/sample - loss: 0.4757 - mae: 0.5008 - mse: 0.4757 - val_loss: 0.5347 - val_mae: 0.5347 - val_mse: 0.5347\n",
      "Epoch 17/1000\n",
      "13209/13209 [==============================] - 1s 62us/sample - loss: 0.4776 - mae: 0.5018 - mse: 0.4776 - val_loss: 0.5209 - val_mae: 0.5169 - val_mse: 0.5209\n",
      "Epoch 18/1000\n",
      "13209/13209 [==============================] - 1s 62us/sample - loss: 0.4774 - mae: 0.5022 - mse: 0.4774 - val_loss: 0.5251 - val_mae: 0.5187 - val_mse: 0.5251\n",
      "Epoch 19/1000\n",
      "13209/13209 [==============================] - 1s 56us/sample - loss: 0.4765 - mae: 0.5006 - mse: 0.4765 - val_loss: 0.5175 - val_mae: 0.5139 - val_mse: 0.5175\n",
      "Epoch 20/1000\n",
      "13209/13209 [==============================] - 1s 60us/sample - loss: 0.4754 - mae: 0.5018 - mse: 0.4754 - val_loss: 0.5216 - val_mae: 0.5175 - val_mse: 0.5216\n",
      "Epoch 21/1000\n",
      "13209/13209 [==============================] - 1s 60us/sample - loss: 0.4759 - mae: 0.5000 - mse: 0.4759 - val_loss: 0.5254 - val_mae: 0.5117 - val_mse: 0.5254\n",
      "Epoch 22/1000\n",
      "13209/13209 [==============================] - 1s 61us/sample - loss: 0.4753 - mae: 0.5014 - mse: 0.4753 - val_loss: 0.5228 - val_mae: 0.5305 - val_mse: 0.5228\n",
      "Epoch 23/1000\n",
      "13209/13209 [==============================] - 1s 62us/sample - loss: 0.4736 - mae: 0.5006 - mse: 0.4736 - val_loss: 0.5169 - val_mae: 0.5229 - val_mse: 0.5169\n",
      "Epoch 24/1000\n",
      "13209/13209 [==============================] - 1s 61us/sample - loss: 0.4741 - mae: 0.5001 - mse: 0.4741 - val_loss: 0.5243 - val_mae: 0.5088 - val_mse: 0.5243\n",
      "Epoch 25/1000\n",
      "13209/13209 [==============================] - 1s 60us/sample - loss: 0.4749 - mae: 0.5003 - mse: 0.4749 - val_loss: 0.5220 - val_mae: 0.5111 - val_mse: 0.5220\n",
      "Epoch 26/1000\n",
      "13209/13209 [==============================] - 1s 59us/sample - loss: 0.4743 - mae: 0.4993 - mse: 0.4743 - val_loss: 0.5258 - val_mae: 0.5262 - val_mse: 0.5258\n",
      "Epoch 27/1000\n",
      "13209/13209 [==============================] - 1s 57us/sample - loss: 0.4740 - mae: 0.5005 - mse: 0.4740 - val_loss: 0.5394 - val_mae: 0.5468 - val_mse: 0.5394\n",
      "Epoch 28/1000\n",
      "13209/13209 [==============================] - 1s 61us/sample - loss: 0.4752 - mae: 0.5006 - mse: 0.4752 - val_loss: 0.5327 - val_mae: 0.5292 - val_mse: 0.5327\n",
      "Epoch 29/1000\n",
      "13209/13209 [==============================] - 1s 60us/sample - loss: 0.4723 - mae: 0.4994 - mse: 0.4723 - val_loss: 0.5308 - val_mae: 0.5325 - val_mse: 0.5308\n",
      "Epoch 30/1000\n",
      "13209/13209 [==============================] - 1s 62us/sample - loss: 0.4756 - mae: 0.5008 - mse: 0.4756 - val_loss: 0.5374 - val_mae: 0.5058 - val_mse: 0.5374\n",
      "Epoch 31/1000\n",
      "13209/13209 [==============================] - 1s 59us/sample - loss: 0.4731 - mae: 0.5001 - mse: 0.4731 - val_loss: 0.5279 - val_mae: 0.5206 - val_mse: 0.5279\n",
      "Epoch 32/1000\n",
      "13209/13209 [==============================] - 1s 60us/sample - loss: 0.4724 - mae: 0.4991 - mse: 0.4724 - val_loss: 0.5295 - val_mae: 0.5241 - val_mse: 0.5295\n",
      "Epoch 33/1000\n",
      "13209/13209 [==============================] - 1s 61us/sample - loss: 0.4731 - mae: 0.4995 - mse: 0.4731 - val_loss: 0.5207 - val_mae: 0.5170 - val_mse: 0.5207\n"
     ]
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lgCZjgOT7JqO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "ehTtTmPT61oH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have now learned how to build a deep neural network to solve a regression problem. We have visualized our loss in order to determine when we might stop training, and we have utilized early stopping to avoid wasting time training a model.\n",
    "\n",
    "Welcome to deep neural networks. They are deceptively simple to build, but they are very complex to master. When you can build a model to fit a domain, you can create amazing predictions that rival human experts."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "21IdNfUG64H6"
   }
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "copyright",
    "5fsbamUAVuCP",
    "0o3jHPj9llA8",
    "s9hGTw7O6Mtu"
   ],
   "include_colab_link": true,
   "name": "Regression with TensorFlow",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('data': conda)"
  },
  "interpreter": {
   "hash": "8fbdc03ba97d8a2a62d8e820f8dc08c72c861e4a4433063471236fa03b6ac1df"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}